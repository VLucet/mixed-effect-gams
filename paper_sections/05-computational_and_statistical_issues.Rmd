
# V: Computational and statistical issues when fitting HGAMs

```{r part_5_premble, include=FALSE}
#### Code for V: Computational and statistical issues when fitting HGAMs ####
```
Which of the five model formulations (Fig. \ref{fig:models}) should you choose for a given data set? There are two major trade-offs to consider. The first is the bias-variance trade-off: more complex models can account for more fluctuations in the data, but also tend to give more variable predictions, and can overfit.  The second trade-off is model complexity versus computational cost: more complex models can include more potential sources of variation and give more information about a given data set, but will generally take more time and computational resources to fit and debug. We discuss both of these trade-offs in this section. We also discuss
how to extend the HGAM framework to fit more complex models.

## Bias-variance trade-offs

The bias-variance trade-off is a fundamental concept in statistics. When trying to estimate any relationship (in the case of GAMs, a smooth relationship between predictors and data) bias measures how far, on average, an estimate is from the true value.
The variance of an estimator corresponds to how much that estimator would fluctuate if applied to multiple different samples of the same size taken from the
same population.
These two properties tend to be traded off when fitting models.
For instance, rather than estimating a population mean from data, we could simply use a predetermined fixed value regardless of the observed data[^mean_note].
This estimate would have no variance (as it is always the same regardless of what the data look like) but would have high bias unless the true population mean happened to equal the fixed value we chose. 
Penalization is useful because using a penalty term slightly increases model bias, but can substantially decrease variance [@efron_steins_1977].

In GAMs, the bias-variance trade-off is managed by the terms of the penalty matrix, and equivalently random effect variances in HGLMs.
Larger penalties correspond to lower variance, as the estimated function is unable to wiggle a great deal, but also correspond to higher bias unless the true function is close to the null space for a given smoother (e.g., a straight line for thin plate splines with 2nd derivative penalties, or zero for a random effect).
The computational machinery used by **mgcv** to fit smooth terms is designed to find penalty terms that best trade-off bias for variance to find a smoother that can effectively predict new data.

The bias-variance trade-off comes into play with HGAMs when choosing whether to
fit separate penalties for each group level or assign a common penalty for all
group levels (i.e., deciding between models *GS* & *GI* or models *S* & *I*). If the
functional relationships we are trying to estimate for different group levels
actually vary in how wiggly they are, setting the penalty for all group-level
smoothers equal (models *GS* & *S*) will either lead to overly variable estimates for the
least variable group levels, over-smoothed (biased) estimates for the most
wiggly terms, or a mixture of these two, depending on the fitting criteria.

```{r single_smooth_bias, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
#This code will generate the bias-variance tradeoff plot
set.seed(1)

#Calculate the numerical approximation of the 2nd derivative for a a function
#given x and y values. Assumes a constant step size (delta) for x along its path.
calc_2nd_deriv = function(x,y){
  deriv_val = (lag(y) + lead(y) - 2*y)/(x-lag(x))^2
  deriv_val
}

#Generate true regression functions that differ in their frequencies. 
#Higher frequencies correspond to more variable functions. 
freq_vals = c(1/2,1,2,4)
n_reps = 25
noise_levels = c(0.5,1,2)
biasvar_data = crossing(noise = noise_levels,
                        rep = 1:n_reps,
                        x = seq(0,2*pi,length=150),
                        freq = freq_vals
               )%>%
  mutate(y = cos(freq*x) +rnorm(n(), 0, noise),
         grp = paste("frequency = ",freq,sep= ""),
         grp = factor(grp,  
                      levels = paste("frequency = ",freq_vals,sep= "")))

biasvar_fit = biasvar_data %>%
  group_by(noise,rep)%>%
  do(
    #Fit model S (shared smoothness) for the test data
    modS = bam(y~s(x,k=30,grp, bs="fs"), data=.),
    #Fit a model I function (differing smoothness) for the test data
    modI = bam(y~s(x,k=30,by=grp)+s(grp,bs="re"), data=.)
    )

#Extract fitted values for each model for all the test data
biasvar_predict_data = crossing(x = seq(0,2*pi,length=500), 
                                freq = freq_vals)%>%
  mutate(grp = paste("frequency = ",freq,sep= ""),
         grp = factor(grp,  
                      levels = paste("frequency = ",freq_vals,sep= "")),
         y = cos(freq*x))

biasvar_predict_fit = biasvar_fit %>%
  group_by(noise,rep)%>%
  do(fitS = as.numeric(predict(.$modS[[1]],
                               newdata = biasvar_predict_data,
                               type = "response")),
     fitI = as.numeric(predict(.$modI[[1]],
                               newdata = biasvar_predict_data,
                               type="response")))%>%
  unnest(fitS, fitI) %>%
  bind_cols(crossing(noise=noise_levels, 
                     rep= 1:n_reps,
                     biasvar_predict_data))

#turn this into long-format data for plotting, and to make it easier to
#calculate derivatives
biasvar_predict_fit_long = biasvar_predict_fit %>%
  gather(model, value, y, fitS, fitI)%>%
  mutate(model = recode(model, y = "true value",fitS = "model S fit",
                        fitI = "model I fit"),
         model = factor(model, levels=  c("true value","model S fit",
                                          "model I fit")))

biasvar_predict_fit_summary = biasvar_predict_fit_long %>%
  group_by(noise,grp,model, x)%>%
  summarize(lower = min(value),
            upper = max(value),
            value = mean(value)
            )
#estimate 2nd derivatives of each curve, then for each curve calculate the sum
#of squared 2nd derivatives of the true curve and the predictions for both
#models.
deriv_est_data = biasvar_predict_fit %>%
  group_by(grp, rep,noise)%>%
  arrange(grp, x)%>%
  mutate(fitS_deriv = calc_2nd_deriv(x,fitS),
         fitI_deriv = calc_2nd_deriv(x,fitI))%>%
  summarize(freq = freq[1], fitS_int = sum(fitS_deriv^2*(x-lag(x)),
                                           na.rm = TRUE),
            fitI_int = sum(fitI_deriv^2*(x-lag(x)),na.rm = TRUE))%>%
  ungroup()%>%
  mutate(sqr_2nd_deriv = freq^3*(sin(4*pi*freq)+4*pi*freq)/4)%>%
  gather(key=model,value = obs_sqr_deriv,fitS_int,fitI_int)%>%
  mutate(model = factor(ifelse(model=="fitS_int", "model S fit",
                               "model I fit"),
                        levels = c("model S fit",
                                   "model I fit")))

```

We developed a simple numerical experiment to determine whether **mgcv**'s fitting
criteria tend to set estimated smoothness penalties high or low in the presence
of among-group variability in smoothness when fitting model *GS* or *S* HGAMs. We
simulated data from four different groups, with all groups having the same
levels of the covariate $x$, equally spaced across the range from 0 to $2\pi$.
For each group, the true function relating $x$ to the response, $y$, was a cosine wave, but the frequency varied from 0.5 (equal to half a cycle across the range of $x$) to 4 (corresponding to 4 full cycles across the range).
As all four sin waves spanned the whole range from -1 to +1 across the range of x, and as they were all integer or half-integer frequencies, the signal for all groups had the same variance across the range of $x$, approximately equal to 0.5. 
Therefore, the true function for all groups had the same strength of signal;
all that varied between groups was how rapidly the signal fluctuated. 
We added normally distributed error to all $y$-values, with three different noise levels, given by standard deviations of 0.5,1, and 2. These correspond to signal-to-noise ratios (i.e. variance of the cosine curve divided by variance of the noise) of 2, 0.5, and 0.125.
For each noise level, we created 25 replicate data sets, to illustrate the amount of simulation-to-simulation variation in model fit. 
We then fit both model *S* (where all curves were assumed to be equally smooth) and model *I* (with varying smoothness) to each replicate for each noise level, using REML criteria to estimate penalties.

A sample of the fits for each group for three of the replicates for each model are shown in Fig. \ref{fig:var_pen}a, with model *S* in red and model *I* in blue. 
Figure \ref{fig:var_pen}b illustrates how well each model fared across the range of replicates at accurately estimating the true smoothness of the highest frequency terms as measured by the squared second derivative of the smooth fit versus that of the true function, with the distance to the black one-to-one line indicating the degree to which the estimated function for each group over- or under-estimated the smoothness of the true signal. 
In general, under low noise conditions (Fig. \ref{fig:var_pen}, signal-to-noise ratio of 2), model *S* tendeded to overfit the smoothest, lowest-frequency, groups, while accurately fitting the highest frequency groups. 
Under moderate signal-to-noise ratios,  model *S* tended to over-penalize high-frequency groups and under-penalize low frequency groups, and in the lowest signal-to-noise ratio tested (0.125), model *S* tended to penalize all groups towards very smooth functions (Fig. \ref{fig:var_pen}b). 
Curves estimated by model *I*, on the other hand, tended to accurately capture the true wiggliness of the function across the whole range of frequencies and noises, except for the lowest-frequency groups, and the highest frequency groups it the presence of high noise; 
in both cases, model *I* tended to over-smooth (Fig. \ref{fig:var_pen}b). 

This implies that assuming equal smoothness will result in underestimating
the true smoothness of low-variability terms in cases of high signal-to-noise, and overestimating the true smoothness of high-frequency terms in low signal-to-noise data sets. 
If this is a potential issue, we recommend fitting both models *S* and *I* and using standard model evaluation criteria (e.g., AIC) or out-of-sample predictive accuracy (as in Section IV) to determine if there is evidence for among-group variability in smoothness.
However, it may be the case that there are too few data points per group to estimate separate smoothness levels, in which case model *GS* or model *S* may still be the better option even in the face of varying smoothness.

The ideal case would be to assume that among-group penalties follow their own distribution (estimated from the data), to allow variation in smoothness while still getting the benefit of pooling information on smoothness between groups.
This is currently not implemented in  **mgcv**.
It is possible to set up this type of varying penalty model in flexible Bayesian modelling software such as *Stan* (see below for a discussion of how to fit HGAMs using these tools), where inter-group variation in smoothing penalties could be modelled with a hierarchical prior. However, to the best of our knowledge, how to fit this type of model has not been well studied in either the Bayesian or frequentist literature. 

[^mean_note]: While this example may seem contrived, this is exactly what happens
when we assume a given regression coefficient is equal to zero (and thus exclude it from a model).

```{r single_smooth_bias_plot, echo=FALSE, fig.width=7, fig.height=7, message=FALSE, warning=FALSE, cache=TRUE, fig.cap="\\label{fig:var_pen} a) Illustration of bias that can arise from assuming equal smoothness for all group levels (model *S*, red lines) versus allowing for intergroup variation in smoothness (model *I*, blue lines) across a range of signal-to-noise ratios, holding the group-level signals constant. The true function for each group level is shown in black. b) Distribution of wiggliness (as measured by the integral of the squared 2nd derivative)  of the estimated function for each replicate for each group level for model *S* (red) and model *I* (blue), versus the true wiggliness of the function for that grouping level, with the black line indicating the one-to-one line. Points below (above) the black line indicate that a given model estimated the curve as less (more) wiggly than the true curve used to generate the data. Estimated wiggliness less than $10^{-3}$ was truncated for visual clarity, as **mgcv** estimated effectively straight lines for several groups, corresponding to a wiggliness of 0, which would not appear on a log-scaled plot.", dependson= -1}

# Uses the Tex function from the latex2exp package to create a math label for 
# the facets. Based off code from
# https://sahirbhatnagar.com/blog/2016/facet_wrap_labels/
noise_labeller <- function(string) {
  signal_to_noise = as.numeric(string)
  signal_to_noise = 0.5/signal_to_noise^2
  signal_to_noise = as.character(signal_to_noise)
  TeX(paste("signal:noise =", signal_to_noise)) 
}

#The derivative plots

deriv_min = -3
deriv_plot =  ggplot(data=deriv_est_data, 
                     aes(x=sqr_2nd_deriv,     
                         y= pmax(obs_sqr_deriv,10^deriv_min),
                         fill= model,
                         group=paste(sqr_2nd_deriv,model)))+
  facet_grid(.~noise, 
             labeller = as_labeller(noise_labeller,
                                    default = label_parsed))+
  geom_dotplot(binaxis = "y", 
               stackdir = "center",
               binwidth = 0.125,
               color=NA)+
  scale_y_log10("Fitted wiggliness",
                limits = c(10^deriv_min,5e+3),expand=c(0,0.1),
                breaks = c(10^deriv_min,  1e+0, 1e+3), 
                labels = list(bquote(""<=10^.(deriv_min)),
                              bquote(10^0), 
                              bquote(10^3))
                )+
  scale_x_log10("True wiggliness", 
                breaks= c(1e-3, 1e+0, 1e+3), 
                labels = list(bquote(10^-3),bquote(10^0), bquote(10^3))
                )+
  scale_fill_brewer(name=NULL,palette= "Set1")+
  geom_abline(color="black")+
  theme(legend.position = "top",
        strip.text.x = element_blank(),
        plot.margin = unit(c(3, 5.5, 5,5, 5.5), "pt"))

fit_colors = c("black",RColorBrewer::brewer.pal(3, "Set1")[1:2])

overfit_vis_plot = ggplot(data=biasvar_predict_fit_summary,
                          aes(x=x,y= value,color=model))+
  facet_grid(grp~noise, labeller = as_labeller(noise_labeller,
                                               default = label_parsed))+
  geom_line(data=filter(biasvar_predict_fit_long,
                        rep==1,
                        model=="true value"),
            color= "black",
            size=0.9)+
  geom_line(data=filter(biasvar_predict_fit_long,rep%in%1:3),
            aes(group=paste(rep,model)))+
  scale_color_manual(name = "",values=fit_colors)+
  scale_y_continuous("Estimated curve", breaks = c(-2,0,2))+
  scale_x_continuous(name = "x")+
  coord_cartesian(ylim=c(-2,2))+
  theme(legend.position = "top",
        strip.text.y = element_blank(),
        plot.margin = unit(c(5.5, 5.5, 2.5, 5.5), "pt"))

#Plot the overfit graphs together.
cowplot::plot_grid(overfit_vis_plot, deriv_plot, ncol=1, labels="auto",
                   align="hv", axis="lr",
                   rel_heights = c(1,0.5))
```

It may seem there is also a bias-variance trade-off between choosing to use a single global smoother (model *G*) or a global smoother plus group-level terms (models *GS* and *GI*).
In model *G*, all the data is used to estimate a single smooth term, and thus should have lower variance than models *GS* and *GI*, but higher bias for any given group in the presence of inter-group functional variability.
However, in practice, this trade-off will be handled via penalization; if there are no average differences between functional responses, **mgcv** will penalize the group-specific functions toward zero, and thus toward the global model.
The choice between using model *G* versus models *GS* and *GI* should generally be driven by computational costs.
Model *G* is typically much faster to fit than models *GS* and *GI*, even in the absence of among-group differences.
If there is no need to estimate inter-group variability, model *G* will typically be
more efficient.

A similar issue exists when choosing between models *GS* and *GI* and models *S* and *I*.
If all group levels have very different functional shapes, the global term will get penalized toward zero in models *GS* and *GI*, so they will reduce to models *S* and *I*.
The choice to include a global term should be made based on scientific considerations (is the global term of interest?) and computational considerations.

## Complexity-computation trade-offs

The more flexible a model is, the larger an effective parameter space any fitting software has to search. It can be surprisingly easy to use massive computational resources trying to fit models to even small datasets. While we typically want to select models based on their fit and our inferential goals, computing resources can often act as an effective upper bound on model complexity. For a given data set, assuming a fixed family and link function, the time taken to estimate an HGAM will depend (roughly) on four factors: *(i)* the number of coefficients to be estimated (and thus the number of basis functions chosen), *(ii)* the number of smoothing parameters to be estimated, *(iii)* whether the model needs to estimate both a global smoother and group-level smoothers, and *(iv)* the algorithm and fitting criteria used to estimate parameters.

The most straightforward factor that will affect the amount of computational resources is the number of parameters in the model. Adding group-level smoothers (moving from model *G* to the other models) means that there will be more regression parameters to estimate. For a dataset with $g$ different groups and $n$ data points, fitting a model with just a global smoother, `y~s(x,k=k)` will require $k$ coefficients, and takes $\mathcal{O}(nk^2)$ operations to evaluate.  Fitting the same data using a group-level smoother (model *S*, `y~s(x,fac,bs="fs",k=k)`) will require $\mathcal{O}(nk^2g^2)$ operations to evaluate.  In effect, adding a group-level smoother will increase computational cost by an order of the number of groups squared. The effect of this is visible in the examples we fit in section III.  Table \ref{tab:comp_time_kable} compares the relative time it takes to compute model *G* versus the other models.

One way to deal with this issue would be to reduce the number of basis functions used when fitting group-level smoothers when the number of groups is large, limiting the flexibility of the model. It can also make sense to use more computationally-efficient basis functions when fitting large data sets, such as P-splines [@wood_p_splines_2017] or cubic splines.  Thin plate splines entail greater computational costs [@wood_generalized_2017].

Including a global smoother (models *GS* and *GI* compared to models *S* and *I*) will not generally substantially affect the number of coefficients that need to be estimated (Table \ref{tab:comp_time_kable}).
Adding a global term will add at most `k` extra terms.
It can be substantially less than that, as **mgcv** drops basis functions from co-linear smoothers to ensure that the model matrix is full rank.

Adding additional smoothing parameters (moving from model *GS* to *GI*, or moving from model *S* to *I*) is more costly than increasing the number of coefficients to estimate, as estimating smoothing parameters is computationally intensive [@wood_fast_2011].
This means that models *GS* and *S* will generally be substantially faster than *GI* and *I* when the number of groups is large, as models *GI* and *I* fit a separate set of penalties for each group level. The effect of this is visible in comparing the time it takes to fit model *GS* to model *GI* (which has a smoother for each group) or models *S* and *I* for the example data (Table \ref{tab:comp_time_kable}). Note that this will not hold in all cases. For instance, model *I* takes less time to fit the bird movement data than model *S* does (Table \ref{tab:comp_time_kable}B).


```{r comp_calc, echo=FALSE,  fig.width=4, fig.height=6, message=FALSE, warning=FALSE, cache=TRUE}
#Note: this code takes quite a long time to run! It's fitting all 10 models. Run
#once if possible, then rely on the cached code. There's a reason it's split off
#from the rest of the chunks of code.

#This function extracts the number of penalties used in a model. For Gamma and
#Gaussian families, you need to remove the penalty (i.e. variance) associated
#with the scale term
get_n_pen  = function(model) {
  family = model$family[[1]]
  if(family %in% c("Gamma","gaussian")){
    capture.output({out_val = nrow(gam.vcomp(model))-1})
  }else{
    capture.output({out_val = nrow(gam.vcomp(model))})
  }
  return(out_val)
}

#Extract the number of coefficients from a fitted GAM
get_n_coef = function(model) length(coef(model))

#Get the number of inner and outer iterations needed to fit the final model
get_n_iter = function(model) model$outer.info$iter
get_n_out_iter = function(model) model$iter

#combine results into a single table
comp_resources = crossing(model_number = c("G","GS","GI","S","I"),
                          data_source = factor(c("CO2","bird_move"),
                                               levels = c("CO2","bird_move")),
                          time = 0, n_smooths = 0,
                          n_coef = 0)%>%
  mutate(model_number = factor(model_number, 
                               levels = c("G","GS","GI","S","I")))


#Fit each model to the example data sets, and calculate run time for them
comp_resources[1,"time"] = system.time(
  CO2_modG <- gam(log(uptake) ~ s(log(conc),k=5,m=2, bs="tp")+
                                s(Plant_uo, k =12,  bs="re"),
                  data= CO2,
                  method="REML",
                  control = list(keepData=TRUE))
  )[3]

comp_resources[2,"time"] = system.time(
  bird_modG <- gam(count ~ te(week,latitude, bs= c("cc", "tp"), k=c(10,10)),
                   data= bird_move, 
                   method="REML", 
                   family= poisson,
                   knots=list(week=c(0, 52)),
                   control = list(keepData=TRUE))
  )[3]

comp_resources[3,"time"] = system.time(
  CO2_modGS <- gam(log(uptake) ~ s(log(conc),k=5,m=2, bs="tp")+
                                 s(log(conc), Plant_uo, k=5, bs="fs",m=1),
                  data= CO2,
                  method="REML",
                  control = list(keepData=TRUE))
  )[3]


comp_resources[4,"time"] = system.time(
  bird_modGS <- gam(count ~ te(week,latitude, bs= c("cc", "tp"),
                              k=c(10,10),m=2)+
                    t2(week, latitude, species, bs=c("cc", "tp", "re"),
                       k=c(10, 10, 6), m=2, full=TRUE),
                   data= bird_move, 
                   method="REML", 
                   family= poisson,
                   control = list(keepData=TRUE))
  )[3]


comp_resources[5,"time"] = system.time(
  CO2_modGI <- gam(log(uptake) ~ s(log(conc),k=5,m=2, bs="tp")+
                                 s(log(conc),by= Plant_uo, k =5,  bs="ts",m=1)+
                                 s(Plant_uo,bs="re",k=12),
                  data= CO2,
                  method="REML",
                  control = list(keepData=TRUE)))[3]



comp_resources[6,"time"] = system.time(
  bird_modGI <- gam(count ~ te(week,latitude, bs= c("cc", "tp"),
                               k=c(10,10),m=2) +
                            te(week,latitude, bs= c("cc", "tp"),
                               k=c(10,10),m=1,by= species),
                   data= bird_move, 
                   method="REML", 
                   family= poisson,
                   control = list(keepData=TRUE)))[3]


comp_resources[7,"time"] = system.time(
  CO2_modS <- gam(log(uptake) ~ s(log(conc), Plant_uo, k=5,  bs="fs",m=2),
                  data= CO2,
                  method="REML",
                  control = list(keepData=TRUE))
  )[3]


comp_resources[8,"time"] = system.time(
  bird_modS <- gam(count ~ t2(week, latitude, species, bs=c("cc", "tp", "re"),
                            k=c(10, 10, 6), m=2, full=TRUE),
                   data= bird_move, 
                   method="REML", 
                   family= poisson,
                   control = list(keepData=TRUE))
)[3]


comp_resources[9,"time"] = system.time(
  CO2_modI <- gam(log(uptake) ~ s(log(conc),by= Plant_uo, k =5,  bs="tp",m=2) +
                                s(Plant_uo,bs="re",k=12),
                  data= CO2,
                  method="REML",
                  control = list(keepData=TRUE))
)[3]

comp_resources[10,"time"] = system.time(
  bird_modI <- gam(count ~ te(week,latitude,by=species, bs= c("cc", "tp"),
                              k=c(10,10),m = 2),
                   data= bird_move, 
                   method="REML", 
                   family= poisson,
                   control = list(keepData=TRUE))
)[3]

#combine all fitted models into a list
comp_resources$model = list(CO2_modG, bird_modG, CO2_modGS, bird_modGS,
                            CO2_modGI, bird_modGI,CO2_modS, bird_modS,
                            CO2_modI, bird_modI)

#Extract all of the information on computer time and resources needed for each
#model
comp_resources = comp_resources %>%
  group_by(model_number, data_source)%>%
  mutate(n_smooths = get_n_pen(model[[1]]),
         n_coef = get_n_coef(model[[1]]),
         n_iter = get_n_iter(model[[1]]),
         n_iter_out = get_n_out_iter(model[[1]]))

```


```{r comp_time, echo=FALSE, fig.width=4, fig.height=6, message=FALSE, warning=FALSE, cache=TRUE}

comp_resources_table =comp_resources %>%
  ungroup()%>%
  arrange(data_source,model_number)%>%
  transmute(data_source =data_source, Model=model_number,
            `Relative Time` = time,`Coefficients` = n_coef,
            `Penalties` = n_smooths
            )%>%
  group_by(data_source) %>%
  mutate(#scales processing time relative to model *G*
         `Relative Time` = `Relative Time`/`Relative Time`[1],
         #rounds to illustrate differences in timing.
         `Relative Time` = ifelse(`Relative Time`<10, 
                                  signif(`Relative Time`,1), 
                                  signif(`Relative Time`, 2)) 
         )%>%
  ungroup() %>%
  dplyr::select( - data_source)
```

```{r comp_time_kable, echo=FALSE, fig.width=4, fig.height=6, message=FALSE, warning=FALSE, cache=TRUE,purl=FALSE}
kable(comp_resources_table,format ="latex", caption="Relative computational time and model complexity for different HGAM formulations of the two example data sets from section III. All times are scaled relative to the length of time model \\emph{G} takes to fit to that data set. The number of coefficients measures the total number of model parameters (including intercepts). The number of smoothers is the total number of unique penalty values estimated by the model.", booktabs = TRUE)%>% 
  #NOTE: change format to "latex" when compiling to pdf, 
  #"html" when compiling html
  kable_styling(full_width = FALSE) %>%
  add_header_above(c(" " = 1," "=1, "Number of terms"=2)) %>%
  group_rows("A. CO2 data", 1,5) %>%
  group_rows("B. Bird movement data", 6,10)
```

## Alternative formulations: `bam()`, `gamm()`, and `gamm4()`

When fitting models with large numbers of groups, it is often possible to speed up computation substantially by using one of the alternative fitting routines available through **mgcv**.

The first option is the function `bam()`, this requires the least changes to existing code written using the `gam()` function. `bam()` is designed to improve  performance when fitting large data sets via two mechanisms. First, it saves on memory needed to compute a given model by using a random subset of the data to calculate the basis functions. It then blocks the data and updates model fit within each block [@wood_generalized_2015]. While this is primarily designed to reduce memory usage, it can also substantially reduce computation time. Second, when using `bam()`'s default `method="fREML"` ("Fast REML") method, you can use the `discrete=TRUE` option: this first bins continuous covariates into a smaller number of discrete values before estimating the model, substantially reducing the amount of computation needed (@Wood2017-iy; see `?mgcv::bam` for more details). Setting up the five model types (Fig. \ref{fig:models}) in `bam()` uses the same code as we have previously covered;
the only difference is that you use the `bam()` instead of `gam()` function, and
have the additional option of discretizing your covariates.

`bam()` has a larger computational overhead than `gam()`, so for small numbers of groups, it can be slower than `gam()` (Fig. \ref{fig:alt_timing}). As the number of groups increases, computational time for `bam()` increases more slowly than for `gam()`; in our simulation tests, when the number of groups is greater than 16, `bam()` can be upward of an order of magnitude faster (Fig. \ref{fig:alt_timing}). Note that `bam()` can be somewhat less computationally stable when estimating these models (i.e., less likely to converge). 
While base `bam()` (not fit using `discrete=TRUE`) is slower than the other approaches shown in Fig. \ref{fig:alt_timing}, that does not imply that `bam()` is  a worse choice in general, as it is designed to avoid memory limitations when working with big data rather than explicitly speeding up model fitting. 
The `bam()` functions would likely show much better relative performance when the number of individuals per group were large (in the hundreds to thousands, compared to the 20 individuals per group used in Fig. \ref{fig:alt_timing}).

The second option is to fit models using one of two dedicated mixed effect
model estimation packages, **nlme** and **lme4**. The **mgcv** package includes the
function `gamm()`, which uses the **nlme** package to estimate the GAM,
automatically handling the transformation of smooth terms into random effects
(and back into basis function representations for plotting and other statistical
analyses). The `gamm4()` function, in the separate **gamm4** package, uses **lme4** in a similar way. Using `gamm()` or `gamm4()` to fit models
rather than `gam()` can substantially speed up computation when the number of
groups is large, as both **nlme** and **lme4** take advantage of the sparse
structure of the random effects, where most basis functions will be zero for
most groups (i.e., any group-specific basis function will only take a non-zero
value for observations in that group level). As with `bam()`, `gamm()` and `gamm4()` are generally slower than `gam()` for fitting HGAMs when the number of group
levels is small (in our simulations, <8 group levels), however they do show
substantial speed improvements even with a moderate number of groups, and were
as fast as or faster to calculate than `bam()` for all numbers of grouping levels
we tested (Fig. \ref{fig:alt_timing})[^parallel].


[^parallel]: It is also possible to speed up both `gam()` and `bam()` by using
multiple processors in parallel, whereas this is not currently possible for
`gamm()` and `gamm4()`. For large numbers of grouping levels, this should speed up
computation as well, at the cost of using more memory. However, computation time
will likely not decline linearly with the number of cores used, since not all
model fitting sets are parallelizable, and performance of cores can vary. As
parallel processing can be complicated and dependent on the type of computer
you are using to configure, we do not go into how to use these methods
here. The help file `?mgcv::mgcv.parallel` explains how to use parallel
computations for `gam()` and `bam()` in detail.

```{r alt_model_timing_plot, echo=FALSE, fig.width=6.5, fig.height=4, message=FALSE, warning=FALSE, cache=TRUE, purl=TRUE, fig.cap = "\\label{fig:alt_timing}Elapsed time to estimate the same model using each of the four approaches. Each data set was generated with 20 observations per group using a unimodal global function and random group-specific functions consisting of an intercept, a quadratic term, and logistic trend for each group. Observation error was normally distributed. Models were fit using model 2: \\texttt{y~s(x, k=10, bs=\"cp\") + s(x,fac, k=10, bs=\"fs\", xt=list(bs=\"cp\"), m=1)}. All models were run on a single core."}

#This code calculates the timing it takes to fit the same model (with varying
#amounts of data) for gam, bam, gamm, and gamm4.

# ensures that each new model parameter set is an extension of the old one
set.seed = 1

n_x = 20
x = seq(-2,2, length=n_x)
n_steps = 7

#setting up blank data frame to put results in.
fit_timing_data = data_frame(n_groups = 2^(1:n_steps),
                             gam=rep(0,length=n_steps),
                             `bam (discrete = FALSE)`= 0,
                             `bam (discrete = TRUE)` = 0,
                             gamm = 0, gamm4 = 0)

fac_all = paste("g", 1:max(fit_timing_data$n_groups),sep = "")

model_coefs_all = data_frame(fac=fac_all)%>%
  mutate(int = rnorm(n(), 0,0.1),
         x2  = rnorm(n(),0,0.2),
         logit_slope = rnorm(n(),0, 0.2))

#for each number of observations, fit all the models and calculate run times for
#them
for(i in 1:n_steps){

  n_g =  fit_timing_data$n_groups[i]
  
  fac_current = fac_all[1:n_g]
  fac_current = factor(fac_current, levels=  unique(fac_current))

  model_coefs = model_coefs_all%>%
    filter(fac %in% fac_current)%>%
    mutate(fac = factor(fac, levels= unique(fac)))
  
  #ensures that each new data set is an extension of the old one
  set.seed = 1 
  
  model_data = crossing(fac=fac_current, x=x)%>%
    left_join(model_coefs)%>%
    mutate(base_func  = dnorm(x)*10,
           indiv_func = int + x^2*x2 +
                        2*(exp(x*logit_slope)/(1+exp(x*logit_slope))-0.5),
           y = base_func + indiv_func + rnorm(n()))
  
  fit_timing_data$gam[i] = system.time(gam(y~s(x,k=10, bs="cp") + 
                                             s(x,fac, k=10, bs="fs", 
                                               xt=list(bs="cp"), m=2),
                             data= model_data, method="REML")
                             )[3]
  
  fit_timing_data$`bam (discrete = FALSE)`[i] = system.time(
    bam(y~s(x,k=10, bs="cp") + 
          s(x,fac, k=10, bs="fs", xt=list(bs="cp"),m=2),
        data= model_data, 
        discrete = FALSE)
    )[3]
  
  fit_timing_data$`bam (discrete = TRUE)`[i] = system.time(
    bam(y~s(x,k=10, bs="cp") + 
          s(x,fac, k=10, bs="fs", xt=list(bs="cp"),m=2),
        data= model_data,
        discrete=TRUE)
    )[3]
  
  
  fit_timing_data$gamm[i] = system.time(
    gamm(y~s(x,k=10, bs="cp") + 
           s(x,fac, k=10, bs="fs", xt=list(bs="cp"),m=2),
         data= model_data)
    )[3]
  
  
  fit_timing_data$gamm4[i] = system.time(
    gamm4(y~s(x,k=10, bs="cp") + 
            s(x,fac, k=10, bs="fs", xt=list(bs="cp"),m=2),
          data= model_data)
    )[3]
}

#Combine all of the timing data into long format, ready for plotting
fit_timing_long = fit_timing_data %>% 
  gather(model, 
         timing, 
         gam,
         `bam (discrete = FALSE)`, 
         `bam (discrete = TRUE)`, gamm, gamm4)%>%
  mutate(model = factor(model, levels = c("gam",
                                         "bam (discrete = FALSE)",
                                         "bam (discrete = TRUE)",
                                         "gamm", 
                                         "gamm4")))


timing_plot = ggplot(aes(n_groups, timing, color=model, linetype= model), 
                     data=fit_timing_long)+
  geom_line()+
  geom_point(show.legend = FALSE)+
  scale_color_manual(name = "Model",
                     values = c("black", 
                                "#1b9e77",
                                "#1b9e77", 
                                "#d95f02", 
                                "#7570b3"))+
  scale_linetype_manual(name = "Model", values =c(1,1,2,1,1)) +
  scale_y_log10("Run time (seconds)", 
                breaks = c(0.1,1,10,100), 
                labels = c("0.1", "1","10", "100"))+
  scale_x_log10("Number of groups", 
                breaks = c(2,8,32,128))+
  guides(color = guide_legend(nrow = 2, byrow = TRUE))+
  theme(legend.position = "top")
timing_plot
```

Both `gamm()` and `gamm4()` require a few changes to model code.
First, there are a few limitations on how you are able to specify the different model types (Fig. \ref{fig:models}) in both frameworks. Factor-smoother interaction (`bs="fs"`) basis setup works in both `gamm()` and `gamm4()`. However, as the **nlme** package does not support crossed random effects, it is not possible to have two factor-smoother interaction terms for the same grouping variable in `gamm()` models (e.g., `y~s(x1, grp, bs="fs")+s(x2, grp, bs="fs")`. These type of crossed random effects are allowed in **gamm4**. The use of `te()` terms are not possible in **gamm4**, due to
issues with how random effects are specified in the **lme4** package, making it impossible to code models where multiple penalties apply to a single basis function. Instead, for multidimensional group-level smoothers, the alternate function `t2()` needs to be used to generate these terms, as it creates tensor products with only a single penalty for each basis function (see `?mgcv::t2` for details on these smoothers, and @wood_straightforward_2012 for the theoretical basis behind this type of tensor product). For instance, model *GS* for the
bird movement data we discussed in section III would need to be coded as:

```
bird_modS <- gamm4(count ~ t2(week, latitude, species, k=c(10, 10, 6), m=2,
                              bs=c("cc", "tp", "re")),
                   data=bird_move, family="poisson")
```

These packages also do not support the same range of families for the dependent
variable; `gamm()` only supports non-Gaussian families by using a fitting method
called penalized quasi-likelihood (PQL) that is slower and not as numerically
stable as the methods used in `gam()`, `bam()`, and `gamm4()`. Non-Gaussian families are well supported by **lme4** (and thus **gamm4**), but can only fit them using marginal likelihood (ML) rather than REML, so may tend to over-smooth relative to `gam()` using REML estimation. Further, neither `gamm()` nor `gamm4()` supports several of the extended families available through **mgcv**, such as zero-inflated, negative binomial, or ordered categorical and multinomial distributions.

## Estimation issues when fitting both global and group-level smoothers

When fitting models with separate global and group-level smoothers (models *GS* and *GI*),
one issue to be aware of is concurvity between the global smoother and group-level
terms. Concurvity measures how well one smooth term can be approximated by some
combination of the other smooth terms in the model (see `?mgcv::concurvity` for
details). For models *GS* and *GI*, the global term is either entirely or almost entirely[^te_concurvity] concurved with the group-level smoothers.
This is because, in the absence of the global smooth term, it
would be possible to recreate that average effect by shifting all the group-level
smoothers so they were centered around the global mean. 

[^te_concurvity]: There is an important caveat here. 
When fitting *GS* models using tensor products in **mgcv**, the global and group-level terms will not be entirely concurve because **mgcv** will automatically drop basis functions from the group-level smoother to ensure that these terms are not perfectly concurve. 
That is, so that no basis function in the global term could be formed from a linear combination of group-level basis functions (see `?mgcv::gam.side` for how terms to be dropped are selected).
Group-level terms fit using `bs="fs"` smoothers will not have any basis functions dropped, as `mgcv` disables checking for side-constraints for these smoothers (since all basis functions are fully penalized for this type of smoother, in principle concurvity should not be an issue; see `?mgcv::smooth.construct.fs.smooth.spec` for details). 
 

In practical terms, this
has the consequence of increasing uncertainty around the global mean relative to
a model with only a global smoother. In some cases, it can result in the estimated
global smoother being close to flat, even in simulated examples with a known
strong global effect. This concurvity issue may also increase the time it takes
to fit these models (for example, compare the time it takes to fit models *GI* and
*I* in Table \ref{tab:comp_time_kable}). These models can still be estimated
because of penalty terms;  all of the methods we have discussed for fitting
model *GS* (factor-smoother terms or random effect tensor products) automatically create a penalty for the null space of the group-level terms, so that only the global term has its own unpenalized null space. Both the REML and ML criteria work
to balance penalties between nested smooth terms (this is why nested random
effects can be fitted). We have observed that **mgcv** still occasionally
finds solutions with simulated data where the global term is over-smoothed.

To avoid this issue, we recommend both careful choice of basis and setting model degrees of freedom so that group-level terms are either slightly less flexible than the global term or have a smaller null space. In the examples in section III, we used smoothers with an unpenalized null space for the global smoother and ones with no null space for the group-level terms. When using thin plate splines, this can be done with splines with a lower order of derivative penalized in the group-level smoothers than the global smoothers, as lower-order TPRS splines have fewer basis functions in the null space. For example, we used `m=2` (penalizing squared second derivatives) for the global smoother, and `m=1` (penalizing squared first derivatives) for group-level smoothers in models *GS* and *GI*. Another option is to use a lower number of basis functions (`k`) for group-level relative to global terms.  This will reduce the maximum flexibility possible in the group-level terms. We do caution that these are just rules of thumb. 

As noted above, interpreting the shape of global terms and group-wise deviations seperately for  *GS* models fit using tensor-product group-level terms is complicated by the fact that **mgcv** will drop some basis-functions from the group-level terms to prevent perfect concurvity. 
For tensor-product smoothers, **mgcv** will generally drop $\le k$ terms from the group-level smoother, where $k$ is the number of basis functions in the global smoother.
The total number of terms dropped will depend on the smoothers used for the global and group-level terms, and will be dropped essentially at random from different grouping levels. 
This means that some groups will have a different range of potential deviations from the global smoother than others.
This has the effect of also somewhat altering the shape of the global smooth relative to what it would be based on model *G* (the average curve through all the data); this will be a larger issue when the number of basis functions in the global smooth and the number of group levels are small. 
We have tested the effect of this issue on our simulated `bird_move` data set and did not find that it lead to substantial bias in estimating the shape of the global smoother, relative to the amount of bias inherent in any smooth estimation method[^global_note] (Figure \ref{fig:global_bias}). As noted in section III, we found that `t2()` tensor product smoothers with full penalties (`full = TRUE` in the `t2` function) for group-level smoothers showed the best performance at recreating the true global function from our simulated `bird_move` data set, compared to other possible types of tensor product. Using `te()` tensor products for the group-level terms lead to the global smoother being heavily smoothed relative to the actual average function, used to simulate the data (Figure \ref{fig:global_bias}).
However, more work on when these models accurately reconstruct global effects is still needed.

[^global_note]: It is also important to consider here that the concept of a "global function" is a bit fuzzy itself, and there are many possible ways to define what a global function is (as we discussed in section III).
In our view, the global function fitted in these models should generally be viewed as a useful summary of an average trend across a wide range of groups. 
The global function would only represent an actual average relationship if the grouping levels were drawn at random from some underlying population, and if there was scientific reasons to believe that individual groups should differ from the mean only via some additive function. 


```{r global_bias_plot, echo=FALSE, fig.width=7, fig.height=3, message=FALSE, warning=FALSE, cache=TRUE, purl=TRUE, fig.cap = "\\label{fig:global_bias}Average global function used for simulating bird_move data set (left) compared to the fitted global function for a *GS* model estimated with either a `te()` smoother (middle) or a `t2()` smoother with `full=TRUE (right)` for group-level terms. Both group-level smoothers used the same model specification as in section III except for the type of tensor product used. Colors indicate the value of the linear predictor of bird density at each location in each week."}

#Load the global function for the bird_move dataset
bird_move_global <- read.csv("../data/bird_move_global.csv")


#Simple te() model 
bird_modGS_te <- gam(count ~ te(week, latitude, bs=c("cc", "tp"),
                                k=c(10, 10), m=2) +
                             te(week, latitude, species, bs=c("cc", "tp", "re"),
                                k=c(10, 10, 6), m=2),
                     data=bird_move, method="REML", family="poisson", 
                     knots = list(week = c(0, 52)))

bird_mod_predict = bird_move_global %>%
  mutate(species = "sp1")%>%
  mutate(`te` = predict(bird_modGS_te,
                                               newdata = ., 
                                               type="terms")[,1],
         `t2` =  predict(bird_modGS,
                                             newdata = ., 
                                             type="terms")[,1])%>%
  mutate(`true global function` = `global_scaled_function`)%>%
  gather(key = model, value =`fitted value`, `te`:`true global function`)%>%
  mutate(model = factor(model, 
                        levels = c("true global function",
                                   "te",
                                   "t2")))


bird_global_fitted_plot <- ggplot(bird_mod_predict, 
                                aes(x=week, 
                                    y=latitude,
                                    fill = `fitted value`))+
  facet_grid(~model)+
  geom_raster()+
  scale_x_continuous(expand = c(0,0))+
  scale_y_continuous(expand = c(0,0))+
  scale_fill_viridis_c(name="linear predictor")+
  theme(legend.position = "bottom")

print(bird_global_fitted_plot)
```

There is currently no way to disable dropping side constraints for these terms in **mgcv**. In cases where accurately estimating the global smoother or group-level deviations is essential, we recommend either fitting model *G*,  *GS* using factor-smooth group-level terms (`bs="fs"`, which can also be used to model multi-dimensional isotropic group-level smoothers), or model *GI*. Alternatively, there is specialized functional regression software such as the `fosr` function in the *refund* package [@scheipl_functional_2014], which does not impose these side constraints; instead the package uses a modified type of tensor-product to ensure that group-level terms sum to zero at each level of the predictor [@scheipl_functional_2014].
See below for more information on functional regression.

## A brief foray into the land of Bayes

As mentioned in section II, the penalty matrix can also be treated as the inverse of a prior covariance matrix for model parameters $\boldsymbol{\beta}$.
Intuitively, the basis functions and penalty we use form a prior (in the informal sense) on how we'd like our model term to behave. REML gives an empirical Bayes estimate of the smooth model [@laird_random-effects_1982], where terms in the
null space of the smoother have improper, flat priors (i.e., any value for these terms are considered equally likely), any terms in the range space are treated as having a multivariate normal distribution, and the penalty terms are treated as having an improper flat prior (see
-@wood_generalized_2017 Section 5.8 for more details on this connection). The large-sample approximation of the posterior Bayesian covariance matrix  [@wood_confidence_2006] for model parameters can be extracted from any fitted `gam()` or `bam()` model with `vcov(model)`. This can in turn be used
to generate samples from the posterior distribution of the model, as the approximate Bayesian covariance matrix already incorporates the uncertainty from having to
estimate the covariance matrix into it [the standard confidence intervals used in
**mgcv** are in fact Bayesian posterior credible intervals, which happen to have good frequentist properties; @wood_confidence_2006; @marra_coverage_2012]. Viewing our GAM as Bayesian is a somewhat unavoidable consequence of the equivalence of random effects and splines --- if we think that there is some true smoother that we wish to estimate, we must take a Bayesian view of our random effects (splines) as we do not think that the true smoother changes each time we collect data [@wood_generalized_2017, Section 5.8].

This also means that HGAMs can be included as components in a more complex
fully Bayesian model. The **mgcv** package includes a function `jagam()` that can
take a specified model formula and automatically convert it into code for the JAGS
(or BUGS) Bayesian statistical packages, which can be adapted by the user to
their own needs.

Similarly, the **brms** package [@burkner_brms:_2017], which can fit complex statistical models using the Bayesian software **Stan** [@carpenter_stan:_2017] allows for the inclusion of **mgcv**-style smooth terms as part of the model specification. The **brms** package does not currently support `te()` tensor products, but does support factor-smooth interactions and `t2()`-style tensor products, which means all of the models fitted in this paper can be fit by **brms**.

## Beyond HGAMs: functional regression

The HGAMs we have discussed are actually a type of *functional regression*, which is an extension of standard regression models to cases where the outcome variable $y_i$ and/or the predictor variables $x_i$ for a given outcome are functions, rather than single variables [@ramsay_functional_2005]. HGAMs as we have described them are a form of function-on-scalar regression [@ramsay_functional_2005; @reiss_fast_2010], where we are trying to estimate a smooth function that varies between grouping levels. Here the "scalar" refers to the grouping level, and the function is the smooth term that varies between levels; in contrast, a standard GAM is a type of scalar-on-scalar regression, as the goal is to use a set of single values (scalars) to estimate each (scalar) response. 

We have deliberately focused our paper on these simpler classes of functional regression model, and chosen to use the term HGAM rather than functional regression, as we believe that this more clearly connects these models to modelling approaches already familiar to ecologists. Further, we consider the unit of analysis to still be individual observations, as compared to functional regression where the unit of analysis is whole functions. For instance, we are interested in applications such as species distribution modelling, where the presence of a given species may be predicted from a sum of several species-specific functions of different environmental variables. 

However, there is an extensive literature dedicated to the estimation of more complex functional regression models for any interested reader (see @morris_functional_2015 and  @greven_general_2017 for a good introduction and overview of more recent work in this field). The `refund` package [@greven_general_2017] uses the statistical machinery of **mgcv** to fit these models, and should be usable by anyone familiar with **mgcv** modelling syntax.
Functional regression is also a major area of study in Bayesian statistics (e.g., @kaufman_bayesian_2010).

