% This file was created with JabRef 2.10.
% Encoding: Cp1252


@Article{abel_nonparametric_2002,
  Title                    = {Nonparametric modeling and spatiotemporal dynamical systems},
  Author                   = {Abel, M.},
  Journal                  = {Arxiv},
  Year                     = {2002},

  Month                    = feb,

  Abstract                 = {In this article, it is described how to use statistical data analysis to obtain models directly from data. The focus is put on finding nonlinearities within a generalized additive model. These models are found by the means of backfitting algorithms or more general versions, like the alternating conditional expectation value method. The method is illustrated by numerically generated data. As an application the example of vortex ripple dynamics, a highly complex fluid-granular system is treated.},
  File                     = {arXiv.org Snapshot:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\4VT5HKC4\\0202058.html:text/html},
  Keywords                 = {generalized additive modeling, nonparametric estimation, project: hierarchical additive model tutorial paper, spatial dynamics, time series and statistics for dynamic systems, unread},
  Urldate                  = {2009-11-02}
}

@incollection{baayen_autocorrelated_2018,
	title = {Autocorrelated errors in experimental data in the language sciences: {some} solutions offered by {Generalized} {Additive} {Mixed} {Models}},
	booktitle = {Mixed-{Effects} {Regression} {Models} in {Linguistics}},
	publisher = {Springer},
	author = {Baayen, R Harald and van Rij, Jacolien and de Cat, Cecile and Wood, Simon},
	year = {2018},
	pages = {49--69},
}

@Article{Bolker:2009cs,
  Title                    = {{Generalized linear mixed models: a practical guide for ecology and evolution}},
  Author                   = {Bolker, Benjamin M and Brooks, Mollie E and Clark, Connie J and Geange, Shane W and Poulsen, John R and Stevens, M Henry H and White, Jada-Simone S},
  Journal                  = {Trends in Ecology {\&} Evolution},
  Year                     = {2009},

  Month                    = mar,
  Number                   = {3},
  Pages                    = {127--135},
  Volume                   = {24}
}

@Book{deBoor:1978wq,
  Title                    = {{A Practical Guide to Splines}},
  Author                   = {{de Boor}, Carl},
  Publisher                = {Springer},
  Year                     = {1978}
}

@Article{chu_search_2008,
  Title                    = {Search for additive nonlinear time series causal models},
  Author                   = {Chu, Tianjiao and Glymour, Clark},
  Journal                  = {J. Mach. Learn. Res.},
  Year                     = {2008},

  Month                    = jun,
  Pages                    = {967--991},
  Volume                   = {9},

  Abstract                 = {Pointwise consistent, feasible procedures for estimating contemporaneous linear causal structure from time series data have been developed using multiple conditional independence tests, but no such procedures are available for non-linear systems. We describe a feasible procedure for learning a class of non-linear time series structures, which we call additive non-linear time series. We show that for data generated from stationary models of this type, two classes of conditional independence relations among time series variables and their lags can be tested efficiently and consistently using tests based on additive model regression. Combining results of statistical tests for these two classes of conditional independence relations and the temporal structure of time series data, a new consistent model specification procedure is able to extract relatively detailed causal information. We investigate the finite sample behavior of the procedure through simulation, and illustrate the application of this method through analysis of the possible causal connections among four ocean indices. Several variants of the procedure are also discussed.},
  File                     = {ACM Snapshot:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\8CRICFZX\\citation.html:text/html;Chu_2008_Search for additive nonlinear time series causal models.pdf:D\:\\Documents\\papers\\indexed_papers\\Chu_2008_Search for additive nonlinear time series causal models.pdf:application/pdf},
  ISSN                     = {1532-4435},
  Keywords                 = {causality, frequentist statistics, generalized additive modeling, important reference, nonparametric estimation, parameter estimation, project: forecasting marine species dynamics, project: hierarchical additive model tutorial paper, read, spline regression, splines, statistics, structural equation modeling, time series and statistics for dynamic systems, unread},
  Urldate                  = {2013-04-18}
}

@Article{chung_avoiding_2011,
  Title                    = {Avoiding boundary estimates in linear mixed models through weakly informative priors},
  Author                   = {Chung, Y. and Rabe-Hesketh, S. and Gelman, A. and Liu, J. and Dorie, V.},
  Journal                  = {working paper},
  Year                     = {2011},
  Pages                    = {284},

  Abstract                 = {Variance parameters in mixed or multilevel models can be difficult to estimate, especially when the number of groups is small. We propose a maximum penalized likelihood approach which is equivalent to estimating variance parameters by their marginal posterior mode, given a weakly informative prior distribution. By choosing the prior from the gamma family with at least 1 degree of freedom, we ensure that the prior density is zero at the boundary and thus the marginal posterior mode of the group-level variance will be positive. The use of a weakly informative prior allows us to stabilize our estimates while remaining faithful to the data.}
}

@Article{clough_generalized_2012,
  Title                    = {A generalized approach to modeling and estimating indirect effects in ecology},
  Author                   = {Clough, Yann},
  Journal                  = {Ecology},
  Year                     = {2012},

  Doi                      = {10.1890/11-1899.1},
  ISSN                     = {0012-9658},
  Url                      = {http://www.esajournals.org/doi/abs/10.1890/11-1899.1}
}

@Article{dray_community_2012,
  Title                    = {Community ecology in the age of multivariate multiscale spatial analysis},
  Author                   = {Dray, St{\'e}phane and P{\'e}lissier, Rapha{\"e}l and Couteron, Pierre and Fortin, Marie-Jos{\'e}e and Legendre, Pierre and Peres-Neto, Pedro R. and Bellier, Edwige and Bivand, Roger and Blanchet, F. Guillaume and De Caceres, Miquel and Dufour, Anne-B{\'e}atrice and Heegaard, Einar and Jombart, Thibaut and Munoz, Fran{\c c}ois and Oksanen, Jari and Thioulouse, Jean and Wagner, Helene H.},
  Journal                  = {Ecological Monographs},
  Year                     = {2012},

  Doi                      = {10.1890/11-1183.1},
  File                     = {Dray_2012_Community ecology in the age of multivariate multiscale spatial analysis.pdf:D\:\\Documents\\papers\\indexed_papers\\Dray_2012_Community ecology in the age of multivariate multiscale spatial analysis.pdf:application/pdf;ESA Snapshot:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\M6NCVAPK\\Dray et al. - 2012 - Community ecology in the age of multivariate multi.html:text/html},
  ISSN                     = {0012-9615},
  Keywords                 = {biodiversity, biodiversity statistics, community ecology, community structure, environmental variability, indexed, multivariate statistics, project: hierarchical additive model tutorial paper, read, spatial correlation, spatial sampling, Spatial scale, spatial statistics, statistics, statistics for dependence, statistics for high dimensional systems},
  Url                      = {http://www.esajournals.org/doi/abs/10.1890/11-1183.1}
}

@Article{dusseldorp_combining_2010,
  Title                    = {Combining an additive and tree-based regression model simultaneously: {STIMA}},
  Author                   = {Dusseldorp, Elise},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2010},

  Month                    = aug,
  Number                   = {3},
  Pages                    = {514--530},
  Volume                   = {19},

  Abstract                 = {Additive models and tree-based regression models are two main classes of statistical models used to predict the scores on a continuous response variable. It is known

that additive models become very complex in the presence of higher order interaction

effects, whereas some tree-based models, such as CART, have problems capturing linear main effects of continuous predictors. To overcome these drawbacks, the regression

trunk model has been proposed: a multiple regression model with main effects and a

parsimonious amount of higher order interaction effects. The interaction effects can be

represented by a small tree: a regression trunk. This article proposes a new algorithm{\textemdash}

Simultaneous Threshold Interaction Modeling Algorithm (STIMA){\textemdash}to estimate a regression trunk model that is more general and more efficient than the initial one (RTA)

and is implemented in the R-package stima. Results from a simulation study show

that the performance of STIMA is satisfactory for sample sizes of 200 or higher. For

sample sizes of 300 or higher, the 0.50 SE rule is the best pruning rule for a regression

trunk in terms of power and Type I error. For sample sizes of 200, the 0.80 SE rule is

recommended. Results from a comparative study of eight regression methods applied

to ten benchmark datasets suggest that STIMA and GUIDE are the best performers

in terms of cross-validated prediction error. STIMA appeared to be the best method for

datasets containing many categorical variables. The characteristics of a regression trunk

model are illustrated using the Boston house price dataset.

Supplemental materials for this article, including the R-package stima, are available online.},
  Doi                      = {10.1198/jcgs.2010.06089},
  File                     = {American Statistical Association - Journal of Computational and Graphical Statistics - 0(0)\:1:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\RUBUMTPD\\jcgs.2010.html:text/html},
  ISSN                     = {1061-8600},
  Keywords                 = {CART, generalized additive modeling, interactions, project: hierarchical additive model tutorial paper, regression, unread},
  Shorttitle               = {Combining an {Additive} and {Tree}-{Based} {Regression} {Model} {Simultaneously}},
  Urldate                  = {2010-08-20}
}

@Article{efron_steins_1977,
  Title                    = {Stein's paradox in statistics},
  Author                   = {Efron, Bradley and Morris, Carl},
  Journal                  = {Scientific American},
  Year                     = {1977},
  Number                   = {5},
  Pages                    = {119--127},
  Volume                   = {236},

  __markedentry            = {[Pedersen:]},
  File                     = {efron_1977_stein's paradox in statistics.pdf:D\:\\Pedersen\\Documents\\papers\\indexed_papers\\efron_1977_stein's paradox in statistics.pdf:application/pdf},
  Keywords                 = {multilevel modeling, penalization, read, regression, regularization, statistics for clustering, hierarchical Bayes},
  Owner                    = {Pedersen},
  Timestamp                = {2018.05.05}
}

@InCollection{durlauf_analysis_2008,
  Title                    = {Analysis of variance},
  Author                   = {Gelman, Andrew},
  Booktitle                = {The {New} {Palgrave} {Dictionary} of {Economics}},
  Publisher                = {Nature Publishing Group},
  Year                     = {2008},

  Address                  = {Basingstoke},
  Edition                  = {2},
  Editor                   = {Durlauf, Steven N. and Blume, Lawrence E.},
  Pages                    = {593--598},

  File                     = {Gelman_2008_Analysis of variance.pdf:D\:\\Documents\\papers\\indexed_papers\\Gelman_2008_Analysis of variance.pdf:application/pdf},
  ISBN                     = {978-0-333-78676-5},
  Keywords                 = {analysis of variance, model selection, multilevel modeling, project: hierarchical additive model tutorial paper, read, statistics},
  Url                      = {http://www.dictionaryofeconomics.com/article?id=pde2008_A000098},
  Urldate                  = {2011-09-08}
}

@Article{gelman_induction_2011,
  Title                    = {Induction and deduction in {Bayesian} data analysis},
  Author                   = {Gelman, Andrew},
  Journal                  = {Rationality, Markets and Morals},
  Year                     = {2011},
  Volume                   = {in press},

  Abstract                 = {The classical or frequentist approach to statistics (in which inference is 
centered on significance testing), is associated with a philosophy in which science is 
deductive and follows Popper{\textquoteright}s doctrine of falsification. In contrast, Bayesian inference 
is commonly associated with inductive reasoning and the idea that a model can be 
dethroned by a competing model but can never be directly falsified by a significance test.
The purpose of this article is to break these associations, which I think are incorrect and 
have been detrimental to statistical practice, in that they have steered falsificationists 
away from the very useful tools of Bayesian inference and have discouraged Bayesians 
from checking the fit of their models. From my experience using and developing 
Bayesian methods in social and environmental science, I have found model checking and 
falsification to be central in the modeling process.}
}

@Article{gelman_weakly_2008,
  Title                    = {A weakly informative default prior distribution for logistic and other regression models},
  Author                   = {Gelman, Andrew},
  Journal                  = {The Annals of Applied Statistics},
  Year                     = {2008},

  Month                    = dec,
  Note                     = {Zentralblatt MATH identifier: 1156.62017; Mathematical Reviews number (MathSciNet): MR2655663},
  Number                   = {4},
  Pages                    = {1360--1383},
  Volume                   = {2},

  Abstract                 = {We propose a new prior distribution for classical (nonhierarchical) logistic regression models, constructed by first scaling all nonbinary variables to have mean 0 and standard deviation 0.5, and then placing independent Student-t prior distributions on the coefficients. As a default choice, we recommend the Cauchy distribution with center 0 and scale 2.5, which in the simplest setting is a longer-tailed version of the distribution attained by assuming one-half additional success and one-half additional failure in a logistic regression. Cross-validation on a corpus of datasets shows the Cauchy class of prior distributions to outperform existing implementations of Gaussian and Laplace priors.},
  Doi                      = {10.1214/08-AOAS191},
  File                     = {Gelman_2008_A weakly informative default prior distribution for logistic and other regressio.pdf:D\:\\Documents\\papers\\indexed_papers\\Gelman_2008_A weakly informative default prior distribution for logistic and other regressio.pdf:application/pdf},
  ISSN                     = {1932-6157},
  Keywords                 = {Cauchy distribution, logistic equation, project: hierarchical additive model tutorial paper, regression, weakly informative priors},
  Language                 = {EN},
  Url                      = {http://projecteuclid.org/euclid.aoas/1231424214},
  Urldate                  = {2011-06-07}
}

@Article{Gelman:2006jh,
  Title                    = {{Multilevel (hierarchical) modeling: what it can and cannot do}},
  Author                   = {Gelman, Andrew},
  Journal                  = {Technometrics},
  Year                     = {2006},

  Month                    = aug,
  Number                   = {3},
  Pages                    = {432--435},
  Volume                   = {48}
}

@Article{gelman_analysis_2005,
  Title                    = {Analysis of variance: {Why} it is more important than ever},
  Author                   = {Gelman, Andrew},
  Journal                  = {The Annals of Statistics},
  Year                     = {2005},

  Month                    = feb,
  Pages                    = {1--53},
  Volume                   = {33},

  Doi                      = {10.1214/009053604000001048},
  File                     = {Gelman_2005_Analysis of variance.pdf:D\:\\Documents\\papers\\indexed_papers\\Gelman_2005_Analysis of variance.pdf:application/pdf},
  ISSN                     = {0090-5364},
  Keywords                 = {analysis of variance, multilevel modeling, parameter estimation, project: hierarchical additive model tutorial paper, read, statistics},
  Shorttitle               = {Analysis of variance?},
  Url                      = {http://projecteuclid.org/Dienst/getRecord?id=euclid.aos/1112967698/},
  Urldate                  = {2011-09-08}
}

@Book{gelman2013bayesian,
  Title                    = {Bayesian {D}ata {A}nalysis, Third Edition},
  Author                   = {Gelman, A. and Carlin, J.B. and Stern, H.S. and Dunson, D.B. and Vehtari, A. and Rubin, D.B.},
  Publisher                = {Taylor \& Francis},
  Year                     = {2013},
  Series                   = {Chapman \& Hall/CRC Texts in Statistical Science},

  ISBN                     = {9781439840955},
  Lccn                     = {2013039507}
}

@Article{gelman_bayesian_2006,
  Title                    = {Bayesian measures of explained variance and pooling in multilevel (hierarchical) models},
  Author                   = {Gelman, Andrew and Pardoe, Iain},
  Journal                  = {Technometrics},
  Year                     = {2006},

  Month                    = may,
  Number                   = {2},
  Pages                    = {241--251},
  Volume                   = {48},

  Abstract                 = {Explained variance (R2) is a familiar summary of the fit of a linear regression and has been generalized in various ways to multilevel (hierarchical) models. The multilevel models that we consider in this article are characterized by hierarchical data structures in which individuals are grouped into units (which themselves might be further grouped into larger units), and variables are measured on individuals and each grouping unit. The models are based on regression relationships at different levels, with the first level corresponding to the individual data and subsequent levels corresponding to between-group regressions of individual predictor effects on grouping unit variables. We present an approach to defining R2 at each level of the multilevel model, rather than attempting to create a single summary measure of fit. Our method is based on comparing variances in a single fitted model rather than with a null model. In simple regression, our measure generalizes the classical adjusted R2. We also discuss a related variance comparison to summarize the degree to which estimates at each level of the model are pooled together based on the level-specific regression relationship, rather than estimated separately. This pooling factor is related to the concept of shrinkage in simple hierarchical models. We illustrate the methods on a dataset of radon in houses within counties using a series of models ranging from a simple linear regression model to a multilevel varying-intercept, varying-slope model.},
  Doi                      = {10.1198/004017005000000517},
  Url                      = {http://pubs.amstat.org/doi/abs/10.1198/004017005000000517},
  Urldate                  = {2011-06-18}
}

@Article{gelman_philosophy_2010,
  Title                    = {Philosophy and the practice of {Bayesian} statistics},
  Author                   = {Gelman, Andrew and Shalizi, Cosma Rohilla},
  Journal                  = {Arxiv},
  Year                     = {2010},

  Month                    = jun,
  Volume                   = {preprint},

  Abstract                 = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.},
  Url                      = {http://arxiv.org/abs/1006.3868},
  Urldate                  = {2010-06-28}
}

@Article{gelman_type_2000,
  Title                    = {Type {S} error rates for classical and {Bayesian} single and multiple comparison procedures},
  Author                   = {Gelman, A. and Tuerlinckx, F.},
  Journal                  = {Computational Statistics},
  Year                     = {2000},
  Number                   = {3},
  Pages                    = {373--390},
  Volume                   = {15},

  File                     = {Gelman_2000_Type S error rates for classical and Bayesian single and multiple comparison.pdf:D\:\\Documents\\papers\\indexed_papers\\Gelman_2000_Type S error rates for classical and Bayesian single and multiple comparison.pdf:application/pdf},
  Keywords                 = {project: hierarchical additive model tutorial paper}
}

@Article{guo_functional_2002,
  Title                    = {Functional mixed effects models},
  Author                   = {Guo, Wensheng},
  Journal                  = {Biometrics},
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {121--128},
  Volume                   = {58},

  Abstract                 = {Summary. In this article, a new class of functional models in which smoothing splines are used to model fixed effects as well as random effects is introduced. The linear mixed effects models are extended to non-parametric mixed effects models by introducing functional random effects, which are modeled as realizations of zero-mean stochastic processes. The fixed functional effects and the random functional effects are modeled in the same functional space, which guarantee the population-average and subject-specific curves have the same smoothness property. These models inherit the flexibility of the linear mixed effects models in handling complex designs and correlation structures, can include continuous covariates as well as dummy factors in both the fixed or random design matrices, and include the nested curves models as special cases. Two estimation procedures are proposed. The first estimation procedure exploits the connection between linear mixed effects models and smoothing splines and can be fitted using existing software. The second procedure is a sequential estimation procedure using Kalman filtering. This algorithm avoids inversion of large dimensional matrices and therefore can be applied to large data sets. A generalized maximum likelihood (GML) ratio test is proposed for inference and model selection. An application to comparison of cortisol profiles is used as an illustration.},
  Doi                      = {10.1111/j.0006-341X.2002.00121.x},
  File                     = {Guo_2002_Functional Mixed Effects Models.pdf:D\:\\Documents\\papers\\indexed_papers\\Guo_2002_Functional Mixed Effects Models.pdf:application/pdf},
  ISSN                     = {1541-0420},
  Keywords                 = {functional mixed effect models, functional response, generalized additive modeling, mixed models, multivariate statistics, project: hierarchical additive model tutorial paper, statistics, statistics for functional data, statistics for high dimensional systems, unread},
  Language                 = {en},
  Urldate                  = {2015-01-12}
}

@Article{handcock_kriging_1994,
  Title                    = {Kriging and splines: an empirical comparison of their predictive performance in some applications: comment},
  Author                   = {Handcock, Mark S. and Meier, Kristen and Nychka, Douglas},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1994},
  Number                   = {426},
  Pages                    = {401--403},
  Volume                   = {89},

  Doi                      = {10.2307/2290838},
  File                     = {Handcock_1994_Kriging and Splines.pdf:D\:\\Documents\\papers\\indexed_papers\\Handcock_1994_Kriging and Splines.pdf:application/pdf},
  ISSN                     = {0162-1459},
  Keywords                 = {Gaussian processes, kriging, prediction, project: hierarchical additive model tutorial paper, skimmed, smoothing, spatial statistics, splines, statistics for dependence},
  Shorttitle               = {Kriging and {Splines}},
  Url                      = {http://www.jstor.org.ezproxy.library.wisc.edu/stable/2290838},
  Urldate                  = {2016-03-10}
}

@Book{Hastie:1990vg,
  Title                    = {{Generalized Additive Models}},
  Author                   = {Hastie, T J and Tibshirani, Robert J},
  Publisher                = {Taylor {\&} Francis},
  Year                     = {1990},
  Series                   = {Monographs on Statistics and Applied Probability}
}

@Article{hazelton_semiparametric_2011,
  Title                    = {Semiparametric regression with shape-constrained penalized splines},
  Author                   = {Hazelton, Martin L. and Turlach, Berwin A.},
  Journal                  = {Computational Statistics \& Data Analysis},
  Year                     = {2011},

  Month                    = oct,
  Number                   = {10},
  Pages                    = {2871--2879},
  Volume                   = {55},

  Abstract                 = {In semiparametric regression models, penalized splines can be used to describe complex, non-linear relationships between the mean response and covariates. In some applications it is desirable to restrict the shape of the splines so as to enforce properties such as monotonicity or convexity on regression functions. We describe a method for imposing such shape constraints on penalized splines within a linear mixed model framework. We employ Markov chain Monte Carlo (MCMC) methods for model fitting, using a truncated prior distribution to impose the requisite shape restrictions. We develop a computationally efficient MCMC sampler by using a correspondingly truncated multivariate normal proposal distribution, which is a restricted version of the approximate sampling distribution of the model parameters in an unconstrained version of the model. We also describe a cheap approximation to this methodology that can be applied for shape-constrained scatterplot smoothing. Our methods are illustrated through two applications, the first involving the length of dugongs and the second concerned with growth curves for sitka spruce trees.},
  Doi                      = {10.1016/j.csda.2011.04.018},
  ISSN                     = {0167-9473},
  Urldate                  = {2012-12-04}
}

@Article{heckman_penalized_2013,
  Title                    = {Penalized regression, mixed effects models and appropriate modelling},
  Author                   = {Heckman, Nancy and Lockhart, Richard and Nielsen, Jason D.},
  Journal                  = {Electronic Journal of Statistics},
  Year                     = {2013},
  Pages                    = {1517--1552},
  Volume                   = {7},

  Abstract                 = {Linear mixed effects methods for the analysis of longitudinal data provide a convenient framework for modelling within-individual correlation across time. Using spline functions allows for flexible modelling of the response as a smooth function of time. A computational connection between linear mixed effects modelling and spline smoothing has resulted in a cross-fertilization of these two fields. The connection has popularized the use of spline functions in longitudinal data analysis and the use of mixed effects software in smoothing analyses. However, care must be taken in exploiting this connection, as resulting estimates of the underlying population mean might not track the data well and associated standard errors might not reflect the true variability in the data. We discuss these shortcomings and suggest some easy-to-compute methods to eliminate them.},
  Doi                      = {10.1214/13-EJS809},
  ISSN                     = {1935-7524},
  Language                 = {EN},
  Urldate                  = {2013-06-04}
}

@Article{hughes_dimension_2012,
  Title                    = {Dimension reduction and alleviation of confounding for spatial generalized linear mixed models},
  Author                   = {Hughes, John and Haran, Murali},
  Journal                  = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  Year                     = {2012},
  Pages                    = {no--no},

  Abstract                 = {Summary. Non-Gaussian spatial data are very common in many disciplines. For instance, count data are common in disease mapping, and binary data are common in ecology. When fitting spatial regressions for such data, one needs to account for dependence to ensure reliable inference for the regression coefficients. The spatial generalized linear mixed model offers a very popular and flexible approach to modelling such data, but this model suffers from two major shortcomings: variance inflation due to spatial confounding and high dimensional spatial random effects that make fully Bayesian inference for such models computationally challenging. We propose a new parameterization of the spatial generalized linear mixed model that alleviates spatial confounding and speeds computation by greatly reducing the dimension of the spatial random effects. We illustrate the application of our approach to simulated binary, count and Gaussian spatial data sets, and to a large infant mortality data set.},
  Copyright                = {{\textcopyright} 2012 Royal Statistical Society},
  Doi                      = {10.1111/j.1467-9868.2012.01041.x},
  Language                 = {en},
  Url                      = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2012.01041.x/abstract},
  Urldate                  = {2012-10-16}
}

@Article{kimeldorf_correspondence_1970,
  Title                    = {A correspondence between {Bayesian} estimation on stochastic processes and smoothing by splines},
  Author                   = {Kimeldorf, George S. and Wahba, Grace},
  Journal                  = {The Annals of Mathematical Statistics},
  Year                     = {1970},

  Month                    = apr,
  Number                   = {2},
  Pages                    = {495--502},
  Volume                   = {41},
  Abstract                 = {Project Euclid - mathematics and statistics online},
  Doi                      = {10.1214/aoms/1177697089},
  ISSN                     = {0003-4851, 2168-8990},
  Language                 = {EN},
  Mrnumber                 = {MR254999},
  Owner                    = {Pedersen},
  Timestamp                = {2018.05.04},
  Url                      = {https://projecteuclid.org/euclid.aoms/1177697089},
  Urldate                  = {2018-05-04},
  Zmnumber                 = {0193.45201}
}

@Article{kolar_semi-parametric_2010,
  Title                    = {Semi-parametric methods for estimating time-varying graph structure},
  Author                   = {Kolar, M.},
  Journal                  = {pre-print},
  Year                     = {2010},

  File                     = {Kolar_2010_Semi-parametric methods for estimating time-varying graph structure.pdf:D\:\\Documents\\papers\\indexed_papers\\Kolar_2010_Semi-parametric methods for estimating time-varying graph structure.pdf:application/pdf},
  Keywords                 = {dynamic network, graph theory, indexed, network, network topology, nonparametric estimation, parameter estimation, project: hierarchical additive model tutorial paper, read, statistics, statistics for dependence, statistics for high dimensional systems, time-varying ecological networks}
}

@Misc{lathrop_madison_2000,
  Title                    = {Madison {Wisonsin} {Lakes} {Zooplankton} 1976 - 1994},

  Author                   = {Lathrop, R. C.},
  Year                     = {2000},

  __markedentry            = {[Pedersen:6]},
  Journal                  = {Environmental Data Initiative},
  Owner                    = {Pedersen},
  Timestamp                = {2018.05.05},
  Url                      = {http://dx.doi.org/10.6073/pasta/ec3d0186753985147d4f283252388e05},
  Urldate                  = {2018-05-05}
}

@PhdThesis{marra_problems_2010,
  Title                    = {Some problems in model specification and inference for generalized additive models},
  Author                   = {Marra, Giampiero},
  School                   = {University of Bath},
  Year                     = {2010},

  Address                  = {Bath},
  Type                     = {Doctoral thesis},

  Abstract                 = {Regression models describing the dependence between a univariate response and a set of covariates play a fundamental role in statistics. In the last two decades, a tremendous effort has been made in developing flexible regression techniques such as generalized additive models (GAMs) with the aim of modellingthe expected value of a response variable as a sum of smooth unspecified functions of predictors. Many nonparametric regression methodologies exist including local-weighted regression and smoothing splines. Here the focus is on penalized regression spline methods which can be viewed as a generalization of smoothing splines with a more flexible choice of bases and penalties. This thesis addresses three issues. First, the problem of model misspecifi-cation is treated by extending the instrumental variable approach to the GAM context. Second, we study the theoretical and empirical properties of the con-fidence intervals for the smooth component functions of a GAM. Third, we consider the problem of variable selection within this flexible class of models.
All results are supported by theoretical arguments and extensive simulation experiments which shed light on the practical performance of the methods discussed in this thesis.}
}

@Book{McCullagh:1989ti,
  Title                    = {{Generalized Linear Models, Second Edition}},
  Author                   = {McCullagh, P and Nelder, John A},
  Publisher                = {CRC Press},
  Year                     = {1989},
  Month                    = aug
}

@Article{McMahon:2007ju,
  Title                    = {{Scales of association: hierarchical linear models and the measurement of ecological systems}},
  Author                   = {McMahon, Sean M and Diez, Jeffrey M},
  Journal                  = {Ecology Letters},
  Year                     = {2007},

  Month                    = jun,
  Number                   = {6},
  Pages                    = {437--452},
  Volume                   = {10}
}

@Article{miller_finite_2014,
  Title                    = {Finite area smoothing with generalized distance splines},
  Author                   = {Miller, David L. and Wood, Simon N.},
  Journal                  = {Environmental and Ecological Statistics},
  Year                     = {2014},
  Pages                    = {1--17},

  Abstract                 = {Most conventional spatial smoothers smooth with respect to the Euclidean distance between observations, even though this distance may not be a meaningful measure of spatial proximity, especially when boundary features are present. When domains have complicated boundaries leakage (the inappropriate linking of parts of the domain which are separated by physical barriers) can occur. To overcome this problem, we develop a method of smoothing with respect to generalized distances, such as within domain distances. We obtain the generalized distances between our points and then use multidimensional scaling to find a configuration of our observations in a Euclidean space of 2 or more dimensions, such that the Euclidian distances between points in that space closely approximate the generalized distances between the points. Smoothing is performed over this new point configuration, using a conventional smoother. To mitigate the problems associated with smoothing in high dimensions we use a generalization of thin plate spline smoothers proposed by Duchon (Constructive theory of functions of several variables, pp 85{\textendash}100, 1977). This general method for smoothing with respect to generalized distances improves on the performance of previous within-domain distance spatial smoothers, and often provides a more natural model than the soap film approach of Wood et al. (J R Stat Soc Ser B Stat Methodol 70(5):931{\textendash}955, 2008). The smoothers are of the linear basis with quadratic penalty type easily incorporated into a range of statistical models.},
  Doi                      = {10.1007/s10651-014-0277-4},
  File                     = {Miller_2014_Finite area smoothing with generalized distance splines.pdf:D\:\\Documents\\papers\\indexed_papers\\Miller_2014_Finite area smoothing with generalized distance splines.pdf:application/pdf},
  ISSN                     = {1352-8505, 1573-3009},
  Keywords                 = {boundary effects, generalized additive modeling, Multidimensional scaling, multivariate statistics, nonparametric estimation, project: hierarchical additive model tutorial paper, read, smoothing, spatial statistics, statistics},
  Language                 = {en},
  Urldate                  = {2014-03-02}
}

@Article{morrissey_search_2014,
  Title                    = {In search of the best methods for multivariate selection analysis},
  Author                   = {Morrissey, Michael B.},
  Journal                  = {Methods in Ecology and Evolution},
  Year                     = {2014},

  Month                    = sep,
  Pages                    = {n/a--n/a},

  Abstract                 = {1.Regression is an important method for characterising the form of natural selection from individual-based data. Many kinds of regression analysis exist, but few are regularly employed in studies of natural selection. I provide an overview of some of the main underused types of regression analysis by applying them all to test analyses of viability selection for lamb traits in Soay sheep (Ovis aries). This exercise highlights known problems with existing methods, uncovers some new ones, and also reveals ways to harness underused methods to get around these problems. 2.I first estimate selection gradients using generalised linear models, combined with recentlypublished methods for obtaining quantitatively interpretable selection gradient estimates from arbitrary regression models of trait-fitness relationships. I then also apply generalised ridge regression, the lasso, and projection-pursuit regression, in each case also deriving selection gradients. I compare inferences of non-linear selection by diagonalisation of the matrix and by projection-pursuit regression. 3.Selection gradient estimates generally correspond across different regression methods. Although there is little evidence for non-linear selection in the test datasets, very problematic aspects of the behaviour of analysis based on diagonalisation of the are apparent. In addition to better-known problems, (i) the direction and magnitude of estimated major axes of quadratic selection are biased toward directions of phenotype that have little variance, and (ii) the magnitudes of selection of major axes of variance-standardised are not themselves interpretable in any standardised way. 4.While all regression-based methods for analysis of selection have useful properties, projectionpursuit regression seems to stand out. This method can: (i) provide both dimensionalityreduction, (ii) be the basis for inference of quantitatively interpretable selection gradients, and (iii) by characterising major axes of selection, rather than of linear or quadratic selection separately, provide biologically-interpretable inference of non-linear selection. This article is protected by copyright. All rights reserved.},
  Copyright                = {This article is protected by copyright. All rights reserved.},
  Doi                      = {10.1111/2041-210X.12259},
  File                     = {Morrissey_2014_In search of the best methods for multivariate selection analysis.pdf:D\:\\Documents\\papers\\indexed_papers\\Morrissey_2014_In search of the best methods for multivariate selection analysis.pdf:application/pdf},
  ISSN                     = {2041-210X},
  Keywords                 = {generalised linear models, generalized additive modeling, model selection, multivariate statistics, penalization, project: hierarchical additive model tutorial paper, regression, regularization, smoothing, statistics for high dimensional systems},
  Language                 = {en},
  Urldate                  = {2014-09-05}
}

@Article{papp_shape-constrained_2014,
  Title                    = {Shape-constrained estimation using nonnegative splines},
  Author                   = {Papp, D{\'a}vid and Alizadeh, Farid},
  Journal                  = {Journal of Computational and Graphical Statistics},
  Year                     = {2014},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {211--231},
  Volume                   = {23},

  Abstract                 = {We consider the problem of nonparametric estimation of unknown smooth functions in the presence of restrictions on the shape of the estimator and on its support using polynomial splines. We provide a general computational framework that treats these estimation problems in a unified manner, without the limitations of the existing methods. Applications of our approach include computing optimal spline estimators for regression, density estimation, and arrival rate estimation problems in the presence of various shape constraints. Our approach can also handle multiple simultaneous shape constraints. The approach is based on a characterization of nonnegative polynomials that leads to semidefinite programming (SDP) and second-order cone programming (SOCP) formulations of the problems. These formulations extend and generalize a number of previous approaches in the literature, including those with piecewise linear and B-spline estimators. We also consider a simpler approach in which nonnegative splines are approximated by splines whose pieces are polynomials with nonnegative coefficients in a nonnegative basis. A condition is presented to test whether a given nonnegative basis gives rise to a spline cone that is dense in the space of nonnegative continuous functions. The optimization models formulated in the article are solvable with minimal running time using off-the-shelf software. We provide numerical illustrations for density estimation and regression problems. These examples show that the proposed approach requires minimal computational time, and that the estimators obtained using our approach often match and frequently outperform kernel methods and spline smoothing without shape constraints. Supplementary materials for this article are provided online.},
  Doi                      = {10.1080/10618600.2012.707343},
  File                     = {Papp_2014_Shape-Constrained Estimation Using Nonnegative Splines.pdf:D\:\\Documents\\papers\\indexed_papers\\Papp_2014_Shape-Constrained Estimation Using Nonnegative Splines.pdf:application/pdf},
  ISSN                     = {1061-8600},
  Keywords                 = {density estimation, generalized additive modeling, project: hierarchical additive model tutorial paper, regression, Shape constraint, smoothing, spline regression, splines, statistics, unread},
  Urldate                  = {2014-12-30}
}

@Article{potvin_statistical_1990,
  Title                    = {The statistical analysis of ecophysiological response curves obtained from experiments involving repeated measures},
  Author                   = {Potvin, C and Lechowicz, M.J. and Tardif, S.},
  Journal                  = {Ecology},
  Year                     = {1990},
  Pages                    = {1389--1400},
  Owner                    = {Pedersen},
  Timestamp                = {2018.05.04}
}

@Article{Ruppert:2009bf,
  Title                    = {{Semiparametric regression during 2003{\textendash}2007}},
  Author                   = {Ruppert, David and Wand, M P and Carroll, Raymond J},
  Journal                  = {Electronic Journal of Statistics},
  Year                     = {2009},
  Number                   = {0},
  Pages                    = {1193--1256},
  Volume                   = {3}
}

@Book{Ruppert:2003uc,
  Title                    = {{Semiparametric Regression}},
  Author                   = {Ruppert, David and Wand, M P and Carroll, R J},
  Publisher                = {Cambridge University Press},
  Year                     = {2003},
  Month                    = jul
}

@Article{sangalli_spatial_2013,
  Title                    = {Spatial spline regression models},
  Author                   = {Sangalli, Laura M. and Ramsay, James O. and Ramsay, Timothy O.},
  Journal                  = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  Year                     = {2013},
  Pages                    = {n/a--n/a},

  Abstract                 = {We describe a model for the analysis of data distributed over irregularly shaped spatial domains with complex boundaries, strong concavities and interior holes. Adopting an approach that is typical of functional data analysis, we propose a spatial spline regression model that is computationally efficient, allows for spatially distributed covariate information and can impose various conditions over the boundaries of the domain. Accurate surface estimation is achieved by the use of piecewise linear and quadratic finite elements.},
  Copyright                = {{\textcopyright} 2013 Royal Statistical Society},
  Doi                      = {10.1111/rssb.12009},
  File                     = {Sangalli_2013_Spatial spline regression models.pdf:D\:\\Documents\\papers\\indexed_papers\\Sangalli_2013_Spatial spline regression models.pdf:application/pdf;Snapshot:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\Z76DSBVZ\\abstract.html:text/html},
  ISSN                     = {1467-9868},
  Keywords                 = {boundary effects, generalized additive modeling, nonparametric estimation, penalization, project: hierarchical additive model tutorial paper, read, regression, smoothing, spatial statistics, spline regression, splines, statistics, statistics for dependence},
  Language                 = {en},
  Urldate                  = {2013-04-02}
}

@Article{stanley_biogeographic_2016,
  Title                    = {Biogeographic, ontogenetic, and environmental variability in larval behaviour of {{American}} Lobster ({{{\emph{Homarus}}}}{\emph{ americanus}})},
  Author                   = {Stanley, R.R.E. and Pedersen, E. J. and Snelgrove, P.V.R.},
  Journal                  = {Marine Ecology Progress Series},
  Year                     = {2016},
  Pages                    = {125-146},
  Volume                   = {553},
  File                     = {D:\\Pedersen\\Documents\\papers\\indexed_papers\\stanley_2016_biogeographic, ontogenetic, and environmental variability in larval behaviour.pdf},
  Keywords                 = {behaviour,behavioural ecology,biogeography,experimental ecology,larval behaviour,lobster juveniles,lobster larvae,movement ecology,random walk,read,vertical distribution,authored},
  Owner                    = {Pedersen},
  Timestamp                = {2018.05.04}
}

@Article{thorson_spatial_2015,
  Title                    = {Spatial factor analysis: {A} new tool for estimating joint species distributions and correlations in species range},
  Author                   = {Thorson, James T. and Scheuerell, Mark D. and Shelton, Andrew O. and See, Kevin E. and Skaug, Hans J. and Kristensen, Kasper},
  Journal                  = {Methods in Ecology and Evolution},
  Year                     = {2015},

  Month                    = feb,
  Pages                    = {n/a--n/a},

  Abstract                 = {Predicting and explaining the distribution and density of species is one of the oldest concerns in ecology. Species distributions can be estimated using geostatistical methods, which estimate a latent spatial variable explaining observed variation in densities, but geostatistical methods may be imprecise for species with low densities or few observations. Additionally, simple geostatistical methods fail to account for correlations in distribution among species, and generally estimate such cross-correlations as a post-hoc exercise. We therefore present spatial factor analysis (SFA), a spatial model for estimating a low-rank approximation to multivariate data, and use it to jointly estimate the joint distribution of multiple species simultaneously. We also derive an analytic estimate of cross-correlations among species from SFA parameters. As a first example, we show that distributions for 10 bird species in the breeding bird survey in 2013 can be parsimoniously represented using only 5 spatial factors. As a second case study, we show that forward-prediction of catches for 20 rockfishes (Sebastes spp.) off the U.S. West Coast is more accurate using spatial factor analysis than analyzing each species individually. Finally, we show that single-species models give a different picture of cross-correlations than joint estimation using SFA. SFA complements a growing list of tools for jointly modelling the distribution of multiple species, and provides a parsimonious summary of cross-correlation without requiring explicit declaration of habitat variables. We conclude by proposing future research that would model species cross-correlations using dissimilarity of species{\textquoteright} traits, and the development of spatial dynamic factor analysis for a low-rank approximation to spatial time-series data. This article is protected by copyright. All rights reserved.},
  Copyright                = {This article is protected by copyright. All rights reserved.},
  Doi                      = {10.1111/2041-210X.12359},
  File                     = {Thorson_2015_Spatial factor analysis.pdf:D\:\\Documents\\papers\\indexed_papers\\Thorson_2015_Spatial factor analysis.pdf:application/pdf},
  ISSN                     = {2041-210X},
  Keywords                 = {Gaussian processes, movement ecology, multivariate statistics, project: hierarchical additive model tutorial paper, random matrices, random processes, read, spatial ecology, spatial statistics, species distribution model, statistics, statistics for clustering, statistics for dependence},
  Language                 = {en},
  Shorttitle               = {Spatial factor analysis},
  Url                      = {http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12359/abstract},
  Urldate                  = {2015-02-25}
}

@Article{vanhatalo_bayesian_2012,
  Title                    = {Bayesian modeling with {Gaussian} processes using the {GPstuff} toolbox},
  Author                   = {Vanhatalo, Jarno and Riihim{\"a}ki, Jaakko and Hartikainen, Jouni and Jyl{\"a}nki, Pasi and Tolvanen, Ville and Vehtari, Aki},
  Journal                  = {arXiv:1206.5754 [cs, stat]},
  Year                     = {2012},

  Month                    = jun,
  Note                     = {arXiv: 1206.5754},

  Abstract                 = {Gaussian processes (GP) are powerful tools for probabilistic modeling purposes. They can be used to define prior distributions over latent functions in hierarchical Bayesian models. The prior over functions is defined implicitly by the mean and covariance function, which determine the smoothness and variability of the function. The inference can then be conducted directly in the function space by evaluating or approximating the posterior process. Despite their attractive theoretical properties GPs provide practical challenges in their implementation. GPstuff is a versatile collection of computational tools for GP models compatible with Linux and Windows MATLAB and Octave. It includes, among others, various inference methods, sparse approximations and tools for model assessment. In this work, we review these tools and demonstrate the use of GPstuff in several models.},
  Url                      = {http://arxiv.org/abs/1206.5754},
  Urldate                  = {2015-05-04}
}

@Article{verbyla_analysis_2002,
  Title                    = {The analysis of designed experiments and longitudinal data by using smoothing splines},
  Author                   = {Verbyla, Arũnas P. and Cullis, Brian R. and Kenward, Michael G. and Welham, Sue J.},
  Journal                  = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  Year                     = {1999},

  Month                    = jan,
  Number                   = {3},
  Pages                    = {269--311},
  Volume                   = {48},
  Abstract                 = {In designed experiments and in particular longitudinal studies, the aim may be to assess the effect of a quantitative variable such as time on treatment effects. Modelling treatment effects can be complex in the presence of other sources of variation. Three examples are presented to illustrate an approach to analysis in such cases. The first example is a longitudinal experiment on the growth of cows under a factorial treatment structure where serial correlation and variance heterogeneity complicate the analysis. The second example involves the calibration of optical density and the concentration of a protein DNase in the presence of sampling variation and variance heterogeneity. The final example is a multienvironment agricultural field experiment in which a yield?seeding rate relationship is required for several varieties of lupins. Spatial variation within environments, heterogeneity between environments and variation between varieties all need to be incorporated in the analysis. In this paper, the cubic smoothing spline is used in conjunction with fixed and random effects, random coefficients and variance modelling to provide simultaneous modelling of trends and covariance structure. The key result that allows coherent and flexible empirical model building in complex situations is the linear mixed model representation of the cubic smoothing spline. An extension is proposed in which trend is partitioned into smooth and non?smooth components. Estimation and inference, the analysis of the three examples and a discussion of extensions and unresolved issues are also presented.},
  Doi                      = {10.1111/1467-9876.00154},
  Timestamp                = {2018.05.04},
  Url                      = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9876.00154},
  Urldate                  = {2018-05-04}
}

@Article{vickers_using_2017,
  Title                    = {Using {{GAMM}} to examine inter-Individual heterogeneity in thermal performance curves for \emph{{{Natrix}} natrix} indicates bet hedging strategy by mothers},
  Author                   = {Vickers, Mathew J. and Aubret, Fabien and Coulon, Aur{\'e}lie},
  Journal                  = {Journal of Thermal Biology},
  Year                     = {2017},

  Month                    = jan,
  Pages                    = {16-23},
  Volume                   = {63},
  Abstract                 = {The thermal performance curve (TPC) illustrates the dependence on body- and therefore environmental- temperature of many fitness-related aspects of ectotherm ecology and biology including foraging, growth, predator avoidance, and reproduction. The typical thermal performance curve model is linear in its parameters despite the well-known, strong, non-linearity of the response of performance to temperature. In addition, it is usual to consider a single model based on few individuals as descriptive of a species-level response to temperature. To overcome these issues, we used generalized additive mixed modeling (GAMM) to estimate thermal performance curves for 73 individual hatchling Natrix natrix grass snakes from seven clutches, taking advantage of the structure of GAMM to demonstrate that almost 16\% of the deviance in thermal performance curves is attributed to inter-individual variation, while only 1.3\% is attributable to variation amongst clutches. GAMM allows precise estimation of curve characteristics, which we used to test hypotheses on tradeoffs thought to constrain the thermal performance curve: hotter is better, the specialist-generalist trade off, and resource allocation/acquisition. We observed a negative relationship between maximum performance and performance breadth, indicating a specialist-generalist tradeoff, and a positive relationship between thermal optimum and maximum performance, suggesting ``hotter is better''. There was a significant difference among matrilines in the relationship between Area Under the Curve and maximum performance - relationship that is an indicator of evenness in acquisition or allocation of resources. As we used unfed hatchlings, the observed matriline effect indicates divergent breeding strategies among mothers, with some mothers provisioning eggs unequally resulting in some offspring being better than others, while other mothers provisioned the eggs more evenly, resulting in even performance throughout the clutch. This observation is reminiscent of bet-hedging strategies, and implies the possibility for intra-clutch variability in the TPCs to buffer N. natrix against unpredictable environmental variability.},
  Doi                      = {10.1016/j.jtherbio.2016.11.003},
  File                     = {D:\\Pedersen\\Documents\\papers\\indexed_papers\\Vickers_2017_Using GAMM to examine inter-individual heterogeneity in thermal performance.pdf},
  ISSN                     = {0306-4565},
  Keywords                 = {generalized additive modeling,mixed models,read,statistics for clustering,statistics for high dimensional systems,hierachical gams},
  Owner                    = {Pedersen},
  Timestamp                = {2018.05.04}
}

@Article{vu_continuous-time_????,
  Title                    = {Continuous-time regression models for longitudinal networks},
  Author                   = {Vu, D.Q. and Asuncion, A.U. and Hunter, D.R. and Smyth, P.},
  Journal                  = {pre-print},

  File                     = {Vu__Continuous-time regression models for longitudinal networks.pdf:D\:\\Documents\\papers\\indexed_papers\\Vu__Continuous-time regression models for longitudinal networks.pdf:application/pdf},
  Keywords                 = {dynamic network, indexed, network, network statistics, network topology, project: hierarchical additive model tutorial paper, Project: network dynamics review, read, statistics, statistics for dependence, statistics for high dimensional systems, survival analysis, time-dependent covariates, time series and statistics for dynamic systems}
}

@Article{westveld_mixed_2011,
  Title                    = {A mixed effects model for longitudinal relational and network data, with applications to international trade and conflict},
  Author                   = {Westveld, Anton H.},
  Journal                  = {The Annals of Applied Statistics},
  Year                     = {2011},

  Month                    = jun,
  Number                   = {2},
  Pages                    = {843--872},
  Volume                   = {5},

  Abstract                 = {The focus of this paper is an approach to the modeling of longitudinal social network or relational data. Such data arise from measurements on pairs of objects or actors made at regular temporal intervals, resulting in a social network for each point in time. In this article we represent the network and temporal dependencies with a random effects model, resulting in a stochastic process defined by a set of stationary covariance matrices. Our approach builds upon the social relations models of Warner, Kenny and Stoto [Journal of Personality and Social Psychology 37 (1979) 1742{\textendash}1757] and Gill and Swartz [Canad. J. Statist. 29 (2001) 321{\textendash}331] and allows for an intra- and inter-temporal representation of network structures. We apply the methodology to two longitudinal data sets: international trade (continuous response) and militarized interstate disputes (binary response).},
  Doi                      = {10.1214/10-AOAS403},
  File                     = {Westveld_2011_A mixed effects model for longitudinal relational and network data, with.pdf:D\:\\Documents\\papers\\indexed_papers\\Westveld_2011_A mixed effects model for longitudinal relational and network data, with.pdf:application/pdf},
  ISSN                     = {1932-6157},
  Keywords                 = {dynamic network, dynamics on graphs, functional mixed effect models, indexed, markov chain Monte Carlo, mixed models, network statistics, network topology, project: hierarchical additive model tutorial paper, Project: network dynamics review, read, statistics, statistics for dependence, statistics for high dimensional systems},
  Language                 = {EN},
  Url                      = {http://projecteuclid.org/euclid.aoas/1310562208},
  Urldate                  = {2012-01-24}
}

@Book{wood_generalized_2017,
  Title                    = {Generalized {Additive} {Models}: {An} {Introduction} with {R}, 2nd {Edition}},
  Author                   = {Wood, Simon N.},
  Publisher                = {CRC Press},
  Year                     = {2017},

  Address                  = {Boco Raton, FL},
  Edition                  = {2nd},
  Month                    = may,
  Note                     = {Google-Books-ID: JTkkDwAAQBAJ},
  Abstract                 = {The first edition of this book has established itself as one of the leading references on generalized additive models (GAMs), and the only book on the topic to be introductory in nature with a wealth of practical examples and software implementation. It is self-contained, providing the necessary background in linear models, linear mixed models, and generalized linear models (GLMs), before presenting a balanced treatment of the theory and applications of GAMs and related models. The author bases his approach on a framework of penalized regression splines, and while firmly focused on the practical aspects of GAMs, discussions include fairly full explanations of the theory underlying the methods. Use of R software helps explain the theory and illustrates the practical application of the methodology. Each chapter contains an extensive set of exercises, with solutions in an appendix or in the book’s R data package gamair, to enable use as a course text or for self-study. Simon N. Wood is a professor of Statistical Science at the University of Bristol, UK, and author of the R package mgcv.},
  ISBN                     = {978-1-4987-2837-9},
  Keywords                 = {confidence interval estimation, penalization, read, statistics for high dimensional systems, generalised additive model},
  Language                 = {en},
  Owner                    = {Pedersen},
  Shorttitle               = {Generalized {Additive} {Models}},
  Timestamp                = {2018.05.04}
}

@Article{wood_fast_2011,
  Title                    = {Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models},
  Author                   = {Wood, Simon N.},
  Journal                  = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  Year                     = {2011},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {3--36},
  Volume                   = {73},

  Abstract                 = {Summary. Recent work by Reiss and Ogden provides a theoretical basis for sometimes preferring restricted maximum likelihood (REML) to generalized cross-validation (GCV) for smoothing parameter selection in semiparametric regression. However, existing REML or marginal likelihood (ML) based methods for semiparametric generalized linear models (GLMs) use iterative REML or ML estimation of the smoothing parameters of working linear approximations to the GLM. Such indirect schemes need not converge and fail to do so in a non-negligible proportion of practical analyses. By contrast, very reliable prediction error criteria smoothing parameter selection methods are available, based on direct optimization of GCV, or related criteria, for the GLM itself. Since such methods directly optimize properly defined functions of the smoothing parameters, they have much more reliable convergence properties. The paper develops the first such method for REML or ML estimation of smoothing parameters. A Laplace approximation is used to obtain an approximate REML or ML for any GLM, which is suitable for efficient direct optimization. This REML or ML criterion requires that Newton{\textendash}Raphson iteration, rather than Fisher scoring, be used for GLM fitting, and a computationally stable approach to this is proposed. The REML or ML criterion itself is optimized by a Newton method, with the derivatives required obtained by a mixture of implicit differentiation and direct methods. The method will cope with numerical rank deficiency in the fitted model and in fact provides a slight improvement in numerical robustness on the earlier method of Wood for prediction error criteria based smoothness selection. Simulation results suggest that the new REML and ML methods offer some improvement in mean-square error performance relative to GCV or Akaike's information criterion in most cases, without the small number of severe undersmoothing failures to which Akaike's information criterion and GCV are prone. This is achieved at the same computational cost as GCV or Akaike's information criterion. The new approach also eliminates the convergence failures of previous REML- or ML-based approaches for penalized GLMs and usually has lower computational cost than these alternatives. Example applications are presented in adaptive smoothing, scalar on function regression and generalized additive model selection.},
  Copyright                = {{\textcopyright} 2010 Royal Statistical Society},
  Doi                      = {10.1111/j.1467-9868.2010.00749.x},
  File                     = {Wood_2011_Fast stable restricted maximum likelihood and marginal likelihood estimation of.pdf:D\:\\Documents\\papers\\indexed_papers\\Wood_2011_Fast stable restricted maximum likelihood and marginal likelihood estimation of.pdf:application/pdf},
  ISSN                     = {1467-9868},
  Keywords                 = {functional mixed effect models, generalized additive modeling, maximum likelihood estimation, mixed models, parameter estimation, project: hierarchical additive model tutorial paper, random processes, restricted maximum likelihood, skimmed, smoothing, statistics for functional data},
  Language                 = {en},
  Urldate                  = {2016-02-28}
}

@Book{Wood:2006vg,
  Title                    = {{Generalized Additive Models}},
  Author                   = {Wood, Simon N},
  Publisher                = {CRC Press},
  Year                     = {2006},
  Month                    = feb,
  Series                   = {An Introduction with R}
}

@Article{wood_thin_2003,
  Title                    = {Thin plate regression splines},
  Author                   = {Wood, Simon N.},
  Journal                  = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  Year                     = {2003},
  Number                   = {1},
  Pages                    = {95--114},
  Volume                   = {65},

  File                     = {Wood_2003_Thin plate regression splines.pdf:D\:\\Documents\\papers\\indexed_papers\\Wood_2003_Thin plate regression splines.pdf:application/pdf},
  Keywords                 = {generalized additive modeling, project: hierarchical additive model tutorial paper, read, regression, smoothing, spline regression, statistics for high dimensional systems, theoretical methods},
  Urldate                  = {2016-01-31}
}

@Article{wood_partially_2001,
  Title                    = {Partially specified ecological models},
  Author                   = {Wood, Simon N.},
  Journal                  = {Ecological Monographs},
  Year                     = {2001},

  Month                    = feb,
  Number                   = {1},
  Pages                    = {1--25},
  Volume                   = {71},

  Abstract                 = {Models are useful when they are compared with data. Whether this comparison should be qualitative or quantitative depends on circumstances, but in many cases some statistical comparison of model and data is useful and enhances objectivity. Unfortunately, ecological dynamic models tend to contain assumptions and simplifications that enhance tractability, promote insight, but spoil model fit, and this can cause difficulties when adopting a statistical approach. Furthermore, the arcane numerical analysis required to fit dynamic models reliably presents an impediment to objective model testing by fitting. This paper presents methods for formulating and fitting partially specified models, which aim to achieve a measure of generality by avoiding some of the irrelevant incidental assumptions that are inevitable in more traditional approaches. This is done by allowing delay differential equation models, difference equation models, and differential equation models to be constructed with part of their structure represented by unknown functions, while part of the structure may contain conventional model elements that contain only unknown parameters. An integrated practical methodology for using such models is presented along with several examples, which include use of models formulated using delay differential equations, discrete difference equations/matrix models, ordinary differential equations, and partial differential equations. The methods also allow better estimation from ecological data by model fitting, since models can be formulated to include fewer unjustified assumptions than would usually be the case if more traditional models were used, while still including as much structure as the modeler believes can be justified by biological knowledge: model structure improves precision, while fewer extraneous assumptions reduce unquantifiable bias.},
  Doi                      = {10.1890/0012-9615(2001)071[0001:PSEM]2.0.CO;2},
  File                     = {ESA Online Journals - PARTIALLY SPECIFIED ECOLOGICAL MODELS:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\9F2I9D9H\\0012-9615(2001)071[0001PSEM]2.0.html:text/html},
  ISSN                     = {0012-9615},
  Keywords                 = {dynamics, generalized additive modeling, important reference, key reference, mechanistic modeling, project: hierarchical additive model tutorial paper, Project: nonparametric functional form paper, read, semi-mechanistic modeling, semiparametric estimation, statistics},
  Urldate                  = {2010-06-14}
}

@Article{wood_gams_2002,
  Title                    = {{GAMs} with integrated model selection using penalized regression splines and applications to environmental modelling},
  Author                   = {Wood, S. N and Augustin, N. H},
  Journal                  = {Ecological Modelling},
  Year                     = {2002},
  Number                   = {2},
  Pages                    = {157--177},
  Volume                   = {157},

  Abstract                 = {Generalized Additive Models (GAMs) have been popularized by the work of Hastie
and Tibshirani (1990) and the availability of user friendly GAM software in Splus.
However, whilst it is flexible and efficient, the GAM framework based on backfitting
with linear smoothers presents some difficulties when it comes to model selection and
inference. On the other hand, the mathematically elegant work ofWahba (1990) and
co-workers on Generalized Spline Smoothing (GSS) provides a rigorous framework
for model selection (Gu and Wahba, 1991) and inference with GAMs constructed
from smoothing splines: but unfortunately these models are computationally very
expensive with operations counts that are of cubic order in the number of data.
A {\textquotedblleft}middle way{\textquotedblright} between these approaches is to construct GAMs using penalized
regression splines (see e.g. Wahba 1980, 1990; Eilers and Marx 1998, Wood 2000).
In this paper we develop this idea and show how GAMs constructed using penalized
regression splines can be used to get most of the practical benefits of GSS models,
including well founded model selection and multi-dimensional smooth terms, with
the ease of use and low computational cost of backfit GAMs. Inference with the
resulting methods also requires slightly fewer approximations than are employed
in the GAM modelling software provided in Splus. This paper presents the basic
mathematical and numerical approach to GAMs implemented in the R package
mgcv, and includes two environmental examples using the methods as implemented
in the package.},
  File                     = {Wood_2002_GAMs with integrated model selection using penalized regression splines and.pdf:D\:\\Documents\\papers\\indexed_papers\\Wood_2002_GAMs with integrated model selection using penalized regression splines and.pdf:application/pdf},
  Keywords                 = {cross-validation, generalized additive modeling, project: forecasting marine species dynamics, project: hierarchical additive model tutorial paper, read, spline regression, splines, statistics}
}

@Article{wood_soap_2008,
  Title                    = {Soap film smoothing},
  Author                   = {Wood, Simon N. and Bravington, Mark V. and Hedley, Sharon L.},
  Journal                  = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  Year                     = {2008},
  Number                   = {5},
  Pages                    = {931--955},
  Volume                   = {70},

  Abstract                 = {Summary. Conventional smoothing methods sometimes perform badly when used to smooth data over complex domains, by smoothing inappropriately across boundary features, such as peninsulas. Solutions to this smoothing problem tend to be computationally complex, and not to provide model smooth functions which are appropriate for incorporating as components of other models, such as generalized additive models or mixed additive models. We propose a class of smoothers that are appropriate for smoothing over difficult regions of 2 which can be represented in terms of a low rank basis and one or two quadratic penalties. The key features of these smoothers are that they do not {\textquoteleft}smooth across{\textquoteright} boundary features, that their representation in terms of a basis and penalties allows straightforward incorporation as components of generalized additive models, mixed models and other non-standard models, that smoothness selection for these model components is straightforward to accomplish in a computationally efficient manner via generalized cross-validation, Akaike's information criterion or restricted maximum likelihood, for example, and that their low rank means that their use is computationally efficient.},
  Copyright                = {{\textcopyright} 2008 Royal Statistical Society},
  Doi                      = {10.1111/j.1467-9868.2008.00665.x},
  File                     = {Snapshot:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\3IQRIWN9\\abstract.html:text/html},
  ISSN                     = {1467-9868},
  Keywords                 = {generalized additive modeling, penalization, project: forecasting marine species dynamics, project: hierarchical additive model tutorial paper, read, regression, smoothing, spatial statistics, statistics},
  Language                 = {en},
  Urldate                  = {2013-06-21}
}

@Article{wood_generalized_2015,
  Title                    = {Generalized additive models for large data sets},
  Author                   = {Wood, Simon N. and Goude, Yannig and Shaw, Simon},
  Journal                  = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  Year                     = {2015},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {139--155},
  Volume                   = {64},

  Abstract                 = {We consider an application in electricity grid load prediction, where generalized additive models are appropriate, but where the data set's size can make their use practically intractable with existing methods. We therefore develop practical generalized additive model fitting methods for large data sets in the case in which the smooth terms in the model are represented by using penalized regression splines. The methods use iterative update schemes to obtain factors of the model matrix while requiring only subblocks of the model matrix to be computed at any one time. We show that efficient smoothing parameter estimation can be carried out in a well-justified manner. The grid load prediction problem requires updates of the model fit, as new data become available, and some means for dealing with residual auto-correlation in grid load. Methods are provided for these problems and parallel implementation is covered. The methods allow estimation of generalized additive models for large data sets by using modest computer hardware, and the grid load prediction problem illustrates the utility of reduced rank spline smoothing methods for dealing with complex modelling problems.},
  Copyright                = {{\textcopyright} 2014 The Authors. Journal of the Royal Statistical Society: Series C Applied Statistics Published by John Wiley \& Sons Ltd on behalf of the Royal Statistical Society., This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.},
  Doi                      = {10.1111/rssc.12068},
  File                     = {Full Text PDF:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\AUPPM753\\Wood et al. - 2015 - Generalized additive models for large data sets.pdf:application/pdf},
  ISSN                     = {1467-9876},
  Keywords                 = {generalized additive modeling, project: hierarchical additive model tutorial paper, regression, skimmed, statistics for functional data, statistics for large datasets},
  Language                 = {en},
  Urldate                  = {2016-03-02}
}

@Article{wood_straightforward_2012,
  Title                    = {Straightforward intermediate rank tensor product smoothing in mixed models},
  Author                   = {Wood, Simon N. and Scheipl, Fabian and Faraway, Julian J.},
  Journal                  = {Statistics and Computing},
  Year                     = {2013},

  Month                    = feb,
  Number                   = {3},
  Pages                    = {341--360},
  Volume                   = {23},

  Abstract                 = {Tensor product smooths provide the natural way of representing smooth interaction terms in regression models because they are invariant to the units in which the covariates are measured, hence avoiding the need for arbitrary decisions about relative scaling of variables. They would also be the natural way to represent smooth interactions in mixed regression models, but for the fact that the tensor product constructions proposed to date are difficult or impossible to estimate using most standard mixed modelling software. This paper proposes a new approach to the construction of tensor product smooths, which allows the smooth to be written as the sum of some fixed effects and some sets of i.i.d. Gaussian random effects: no previously published construction achieves this. Because of the simplicity of this random effects structure, our construction is useable with almost any flexible mixed modelling software, allowing smooth interaction terms to be readily incorporated into any Generalized Linear Mixed Model. To achieve the computationally convenient separation of smoothing penalties, the construction differs from previous tensor product approaches in the penalties used to control smoothness, but the penalties have the advantage over several alternative approaches of being explicitly interpretable in terms of function shape. Like all tensor product smoothing methods, our approach builds up smooth functions of several variables from marginal smooths of lower dimension, but unlike much of the previous literature we treat the general case in which the marginal smooths can be any quadratically penalized basis expansion, and there can be any number of them. We also point out that the imposition of identifiability constraints on smoothers requires more care in the mixed model setting than it would in a simple additive model setting, and show how to deal with the issue. An interesting side effect of our construction is that an ANOVA-decomposition of the smooth can be read off from the estimates, although this is not our primary focus. We were motivated to undertake this work by applied problems in the analysis of abundance survey data, and two examples of this are presented.},
  Doi                      = {10.1007/s11222-012-9314-z},
  File                     = {Wood_2012_Straightforward intermediate rank tensor product smoothing in mixed models.pdf:D\:\\Documents\\papers\\indexed_papers\\Wood_2012_Straightforward intermediate rank tensor product smoothing in mixed models.pdf:application/pdf},
  ISSN                     = {0960-3174, 1573-1375},
  Keywords                 = {functional mixed effect models, generalized additive modeling, interactions, mixed models, multivariate statistics, project: hierarchical additive model tutorial paper, read, regression, tensor methods},
  Language                 = {en},
  Urldate                  = {2016-03-21}
}


@article{wood_lowrank_2006,
	title = {Low-rank scale-invariant tensor product smooths for generalized additive mixed models},
	volume = {62},
	issn = {1541-0420},
	doi = {10.1111/j.1541-0420.2006.00574.x},
	language = {en},
	number = {4},
	urldate = {2018-05-31},
	journal = {Biometrics},
	author = {Wood, Simon N.},
	year = {2006},
	pages = {1025--1036}
}

@article{marra_coverage_2012,
	title = {Coverage properties of confidence intervals for generalized additive model components},
	volume = {39},
	issn = {1467-9469},
	doi = {10.1111/j.1467-9469.2011.00760.x},
	language = {en},
	number = {1},
	urldate = {2017-10-14},
	journal = {Scandinavian Journal of Statistics},
	author = {Marra, Giampiero and Wood, Simon N.},
	month = mar,
	year = {2012},
	pages = {53--74}
}

@article{bates_fitting_2015,
	title = {Fitting linear mixed-effects models using lme4},
	volume = {67},
	number = {1},
	urldate = {2014-11-24},
	journal = {Journal of Statistical Software},
	author = {Bates, Douglas and Mächler, Martin and Bolker, Ben and Walker, Steve},
	year = {2015},
	pages = {1--48}
}


@article{wood_smoothing_2016,
	title = {Smoothing parameter and model selection for general smooth models},
	volume = {111},
	issn = {0162-1459},
	doi = {10.1080/01621459.2016.1180986},
	number = {516},
	urldate = {2017-05-29},
	journal = {Journal of the American Statistical Association},
	author = {Wood, Simon N. and Pya, Natalya and Säfken, Benjamin},
	month = oct,
	year = {2016},
	pages = {1548--1563}
}


@article{scheipl_functional_2014,
	title = {Functional additive mixed models},
	volume = {24},
	issn = {10.1080/10618600.2014.901914},
	url = {http://www.tandfonline.com/doi/suppl/10.1080/10618600.2014.901914?scroll=top},
	abstract = {We propose an extensive framework for additive regression models for correlated functional responses, allowing for multiple partially nested or crossed functional random effects with flexible correlation structures for, for example, spatial, temporal, or longitudinal functional data. Additionally, our framework includes linear and nonlinear effects of functional and scalar covariates that may vary smoothly over the index of the functional response. It accommodates densely or sparsely observed functional responses and predictors which may be observed with additional error and includes both spline-based and functional principal component-based terms. Estimation and inference in this framework is based on standard additive mixed models, allowing us to take advantage of established methods and robust, flexible algorithms. We provide easy-to-use open source software in the pffr() function for the R package refund. Simulations show that the proposed method recovers relevant effects reliably, handles small sampl...},
	language = {en},
	number = {2},
	urldate = {2017-09-06},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Scheipl, Fabian and Staicu, Ana-Maria and Greven, Sonja},
	month = apr,
	year = {2014},
	pages = {477--501}
}


@article{reiss_fast_2010,
	title = {Fast function-on-scalar regression with penalized basis expansions},
	volume = {6},
	issn = {15574679},
	url = {https://www.degruyter.com/view/j/ijb.2010.6.1/ijb.2010.6.1.1246/ijb.2010.6.1.1246.xml},
	doi = {10.2202/1557-4679.1246},
	number = {1},
	urldate = {2018-08-02},
	journal = {The International Journal of Biostatistics},
	author = {Reiss, Philip T. and Huang, Lei and Mennes, Maarten},
	year = {2010}
}

@book{ramsay_functional_2005,
	address = {New York, NY},
	edition = {2nd},
	series = {Springer {Series} in {Statistics}},
	title = {Functional {D}ata {A}nalysis},
	publisher = {Springer Science+Business Media, Inc.},
	author = {Ramsay, James and Silverman, B.W.},
	year = {2005}
}

@article{fang_asymptotic_2011,
	title = {Asymptotic equivalence between cross-validations and {Akaike} information criteria in mixed-effects models},
	volume = {9},
	number = {1},
	journal = {Journal of data science},
	author = {Fang, Yixin},
	year = {2011},
	pages = {15--21}
}

@incollection{forster_aic_2011,
	address = {Boston, MA},
	series = {Handbook of the {Philosophy} of {Science}},
	title = {{AIC} scores as evidence: a {Bayesian} interpretation},
	number = {7},
	booktitle = {Philosophy of {Statistics}},
	publisher = {Elsevier B.V.},
	author = {Forster, Malcolm and Sober, Elliott},
	editor = {Bandyopadhyay, Prasanta S. and Forster, Malcolm R.},
	year = {2011},
	pages = {535--549}
}


@article{scheipl_generalized_2016,
	title = {Generalized functional additive mixed models},
	volume = {10},
	doi = {10.1214/16-EJS1145},
	number = {1},
	journal = {Electronic Journal of Statistics},
	author = {Scheipl, Fabian and Gertheiss, Jan and Greven, Sonja},
	year = {2016},
	pages = {1455--1492}
}

@article{wieling_investigating_2016,
	title = {Investigating dialectal differences using articulography},
	volume = {59},
	journal = {Journal of Phonetics},
	author = {Wieling, Martijn and Tomaschek, Fabian and Arnold, Denis and Tiede, Mark and Bröker, Franziska and Thiele, Samuel and Wood, Simon N and Baayen, R Harald},
	year = {2016},
	pages = {122--143}
}

@article{wood_confidence_2006,
	title = {On confidence intervals for generalized additive models based on penalized regression splines},
	volume = {48},
	number = {4},
	journal = {Australian \& New Zealand Journal of Statistics},
	author = {Wood, Simon N},
	year = {2006},
	pages = {445--464},
}

@article{burkner_brms:_2017,
	title = {\texttt{brms}: {An} {R} package for {Bayesian} multilevel models using {Stan}},
	volume = {80},
	number = {1},
	journal = {Journal of Statistical Software},
	author = {Bürkner, Paul-Christian},
	year = {2017},
	pages = {1--28}
}

@article{carpenter_stan:_2017,
	title = {Stan: {A} probabilistic programming language},
	volume = {76},
	number = {1},
	journal = {Journal of statistical software},
	author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
	year = {2017}
}

@article{kaufman_bayesian_2010,
	title = {Bayesian functional {ANOVA} modeling using Gaussian process prior distributions},
	volume = {5},
	number = {1},
	journal = {Bayesian Analysis},
	author = {Kaufman, Cari G and Sain, Stephan R and {others}},
	year = {2010},
	pages = {123--149}
}



@article{wood_p_splines_2017,
	title = {P-splines with derivative based penalties and tensor product smoothing of unevenly distributed data},
	volume = {27},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-016-9666-x},
	doi = {10.1007/s11222-016-9666-x},
	abstract = {The P-splines of Eilers and Marx (Stat Sci 11:89–121, 1996) combine a B-spline basis with a discrete quadratic penalty on the basis coefﬁcients, to produce a reduced rank spline like smoother. P-splines have three properties that make them very popular as reduced rank smoothers: (i) the basis and the penalty are sparse, enabling efﬁcient computation, especially for Bayesian stochastic simulation; (ii) it is possible to ﬂexibly ‘mix-and-match’ the order of B-spline basis and penalty, rather than the order of penalty controlling the order of the basis as in spline smoothing; (iii) it is very easy to set up the B-spline basis functions and penalties. The discrete penalties are somewhat less interpretable in terms of function shape than the traditional derivative based spline penalties, but tend towards penalties proportional to traditional spline penalties in the limit of large basis size. However part of the point of P-splines is not to use a large basis size. In addition the spline basis functions arise from solving functional optimization problems involving derivative based penalties, so moving to discrete penalties for smoothing may not always be desirable. The purpose of this note is to point out that the three properties of basis-penalty sparsity, mix-and-match penalization and ease of setup are readily obtainable with B-splines subject to derivative based penalization. The penalty setup typically requires a few lines of code, rather than the two lines typically required for P-splines, but this one off disadvantage seems to be the only one associated with using derivative based penalties. As an example application, it is shown how basis-penalty sparsity enables efﬁcient computation with tensor product smoothers of scattered data.},
	language = {en},
	number = {4},
	urldate = {2018-04-24},
	journal = {Statistics and Computing},
	author = {Wood, Simon N.},
	month = jul,
	year = {2017},
	pages = {985--989},
}

@article{laird_random-effects_1982,
	title = {Random-effects models for longitudinal data},
	journal = {Biometrics},
	author = {Laird, Nan M and Ware, James H},
	year = {1982},
  volume=38,
  number={4},
	pages = {963--974}
}

@article{pedersen_long-term_2017,
	title = {Long-term growth trends in northern {Wisconsin} walleye populations under changing biotic and abiotic conditions},
	volume = {Early online},
	doi = {doi.org/10.1139/cjfas-2017-0084},
	journal = {Canadian Journal of Fisheries and Aquatic Sciences},
	author = {Pedersen, Eric J and Goto, Daisuke and Gaeta, Jereme W and Hansen, Gretchen JA and Sass, Greg G and Vander Zanden, M Jake and Cichosz, Thomas A and Rypel, Andrew L},
	year = {2017}
}

@article{jonsen_joint_2016,
	title = {Joint estimation over multiple individuals improves behavioural state inference from animal movement data},
	volume = {6},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/srep20625},
	doi = {10.1038/srep20625},
	urldate = {2016-03-06},
	journal = {Scientific Reports},
	author = {Jonsen, Ian},
	month = feb,
	year = {2016},
	pages = {20625}
}

@article{simpson_modelling_2018,
	title = {Modelling palaeoecological time series using generalized additive models},
	url = {http://biorxiv.org/content/early/2018/08/16/322248.abstract},
	doi = {10.1101/322248},
	abstract = {In the absence of annual laminations, time series generated from lake sediments or other similar stratigraphic sequences are irregularly spaced in time, which complicates formal analysis using classical statistical time series models. In lieu, statistical analyses of trends in palaeoenvironmental time series, if done at all, have typically used simpler linear regressions or (non-) parametric correlations with little regard for the violation of assumptions that almost surely occurs due to temporal dependencies in the data or that correlations do not provide estimates of the magnitude of change, just whether or not there is a linear or monotonic trend. Alternative approaches have used LOESS-estimated trends to justify data interpretations or test hypotheses as to the causal factors without considering the inherent subjectivity of the choice of parameters used to achieve the LOESS fit (e.g. span width, degree of polynomial). Generalized additive models (GAMs) are statistical models that can be used to estimate trends as smooth functions of time. Unlike LOESS, GAMs use automatic smoothness selection methods to objectively determine the complexity of the fitted trend, and as formal statistical models, GAMs, allow for potentially complex, non-linear trends, a proper accounting of model uncertainty, and the identification of periods of significant temporal change. Here, I present a consistent and modern approach to the estimation of trends in palaeoenvironmental time series using GAMs, illustrating features of the methodology with two example time series of contrasting complexity; a 150-year bulk organic matter δ15N time series from Small Water, UK, and a 3000-year alkenone record from Braya-Sø, Greenland. I discuss the underlying mechanics of GAMs that allow them to learn the shape of the trend from the data themselves and how simultaneous confidence intervals and the first derivatives of the trend are used to properly account for model uncertainty and identify periods of change. It is hoped that by using GAMs greater attention is paid to the statistical estimation of trends in palaeoenvironmental time series leading to more a robust and reproducible palaeoscience.},
	journal = {bioRxiv},
	author = {Simpson, Gavin L},
	month = jan,
	year = {2018}
}

@ARTICLE{Wood2017-iy,
  title     = "Generalized Additive Models for Gigadata: Modeling the {U.K}.
               Black Smoke Network Daily Data",
  author    = "Wood, Simon N and Li, Zheyuan and Shaddick, Gavin and Augustin,
               Nicole H",
  journal   = "Journal of the American Statistical Association",
  publisher = "Taylor \& Francis",
  volume    =  112,
  number    =  519,
  pages     = "1199--1210",
  month     =  jul,
  year      =  2017,
  eprint    = "http://dx.doi.org/10.1080/01621459.2016.1195744",
  issn      = "0162-1459",
  doi       = "10.1080/01621459.2016.1195744"
}

@book{burnham_model_1998,
	address = {New York, NY},
	title = {Model selection and inference: a practical information-theoretic approach},
	isbn = {978-1-4757-2917-7},
	shorttitle = {Model {Selection} and {Inference}},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Burnham, Kenneth P. and Anderson, David R.},
	year = {1998},
	note = {Google-Books-ID: W63hBwAAQBAJ}
}

@article{greven_general_2017,
	title = {A general framework for functional regression modelling},
	volume = {17},
	issn = {1471-082X},
	url = {https://doi.org/10.1177/1471082X16681317},
	doi = {10.1177/1471082X16681317},
	abstract = {Abstract, Researchers are increasingly interested in regression models for functional data. This article discusses a comprehensive framework for additive (mixed) models for functional responses and/or functional covariates based on the guiding principle of reframing functional regression in terms of corresponding models for scalar data, allowing the adaptation of a large body of existing methods for these novel tasks. The framework encompasses many existing as well as new models. It includes regression for ‘generalized’ functional data, mean regression, quantile regression as well as generalized additive models for location, shape and scale (GAMLSS) for functional data. It admits many flexible linear, smooth or interaction terms of scalar and functional covariates as well as (functional) random effects and allows flexible choices of bases—particularly splines and functional principal components—and corresponding penalties for each term. It covers functional data observed on common (dense) or curve-specific (sparse) grids. Penalized-likelihood-based and gradient-boosting-based inference for these models are implemented in R packages refund and FDboost, respectively. We also discuss identifiability and computational complexity for the functional regression models covered. A running example on a longitudinal multiple sclerosis imaging study serves to illustrate the flexibility and utility of the proposed model class. Reproducible code for this case study is made available online.},
	language = {en},
	number = {1-2},
	urldate = {2018-12-02},
	journal = {Statistical Modelling},
	author = {Greven, Sonja and Scheipl, Fabian},
	month = feb,
	year = {2017},
	pages = {1--35}}
}


@article{morris_functional_2015,
	title = {Functional regression},
	volume = {2},
	issn = {2326-8298},
	url = {https://doi.org/10.1146/annurev-statistics-010814-020413},
	doi = {10.1146/annurev-statistics-010814-020413},
	abstract = {Functional data analysis (FDA) involves the analysis of data whose ideal units of observation are functions defined on some continuous domain, and the observed data consist of a sample of functions taken from some population, sampled on a discrete grid. Ramsay \& Silverman's (1997) textbook sparked the development of this field, which has accelerated in the past 10 years to become one of the fastest growing areas of statistics, fueled by the growing number of applications yielding this type of data. One unique characteristic of FDA is the need to combine information both across and within functions, which Ramsay and Silverman called replication and regularization, respectively. This article focuses on functional regression, the area of FDA that has received the most attention in applications and methodological development. First, there is an introduction to basis functions, key building blocks for regularization in functional regression methods, followed by an overview of functional regression methods, split into three types: (a) functional predictor regression (scalar-on-function), (b) functional response regression (function-on-scalar), and (c) function-on-function regression. For each, the role of replication and regularization is discussed and the methodological development described in a roughly chronological manner, at times deviating from the historical timeline to group together similar methods. The primary focus is on modeling and methodology, highlighting the modeling structures that have been developed and the various regularization approaches employed. The review concludes with a brief discussion describing potential areas of future development in this field.},
	number = {1},
	urldate = {2018-12-01},
	journal = {Annual Review of Statistics and Its Application},
	author = {Morris, Jeffrey S.},
	month = apr,
	year = {2015},
	pages = {321--359}
}

@article{dormann_model_2018,
	title = {Model averaging in ecology: a review of {Bayesian}, information-theoretic, and tactical approaches for predictive inference},
	volume = {88},
	copyright = {© 2018 by the Ecological Society of America},
	issn = {1557-7015},
	shorttitle = {Model averaging in ecology},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecm.1309},
	doi = {10.1002/ecm.1309},
	abstract = {In ecology, the true causal structure for a given problem is often not known, and several plausible models and thus model predictions exist. It has been claimed that using weighted averages of these models can reduce prediction error, as well as better reflect model selection uncertainty. These claims, however, are often demonstrated by isolated examples. Analysts must better understand under which conditions model averaging can improve predictions and their uncertainty estimates. Moreover, a large range of different model averaging methods exists, raising the question of how they differ in their behaviour and performance. Here, we review the mathematical foundations of model averaging along with the diversity of approaches available. We explain that the error in model-averaged predictions depends on each model's predictive bias and variance, as well as the covariance in predictions between models, and uncertainty about model weights. We show that model averaging is particularly useful if the predictive error of contributing model predictions is dominated by variance, and if the covariance between models is low. For noisy data, which predominate in ecology, these conditions will often be met. Many different methods to derive averaging weights exist, from Bayesian over information-theoretical to cross-validation optimized and resampling approaches. A general recommendation is difficult, because the performance of methods is often context dependent. Importantly, estimating weights creates some additional uncertainty. As a result, estimated model weights may not always outperform arbitrary fixed weights, such as equal weights for all models. When averaging a set of models with many inadequate models, however, estimating model weights will typically be superior to equal weights. We also investigate the quality of the confidence intervals calculated for model-averaged predictions, showing that they differ greatly in behaviour and seldom manage to achieve nominal coverage. Our overall recommendations stress the importance of non-parametric methods such as cross-validation for a reliable uncertainty quantification of model-averaged predictions.},
	language = {en},
	number = {4},
	urldate = {2018-12-02},
	journal = {Ecological Monographs},
	author = {Dormann, Carsten F. and Calabrese, Justin M. and Guillera‐Arroita, Gurutzeta and Matechou, Eleni and Bahn, Volker and Bartoń, Kamil and Beale, Colin M. and Ciuti, Simone and Elith, Jane and Gerstner, Katharina and Guelat, Jérôme and Keil, Petr and Lahoz‐Monfort, José J. and Pollock, Laura J. and Reineking, Björn and Roberts, David R. and Schröder, Boris and Thuiller, Wilfried and Warton, David I. and Wintle, Brendan A. and Wood, Simon N. and Wüest, Rafael O. and Hartig, Florian},
	month = nov,
	year = {2018},
	pages = {485--504}
}

@Manual{simpson_gratia_2018,
    title = {gratia: Graceful ggplot-based Graphics and Other Useful Functions for
GAMs Fitted Using mgcv},
    author = {Gavin L. Simpson},
    year = {2018},
    note = {R package version 0.1-9},
    url = {https://gavinsimpson.github.io/gratia},
  }
}

@Book{wickham_ggplot2_2016,
    author = {Hadley Wickham},
    title = {ggplot2: Elegant Graphics for Data Analysis},
    publisher = {Springer-Verlag New York},
    year = {2016},
    isbn = {978-3-319-24277-4},
    url = {http://ggplot2.org},
}
  
@article{marra_practical_2011,
	title = {Practical variable selection for generalized additive models},
	volume = {55},
	issn = {0167-9473},
	doi = {10.1016/j.csda.2011.02.004},
	language = {English},
	number = {7},
	urldate = {2019-01-12},
	journal = {Computational Statistics \& Data Analysis},
	author = {Marra, Giampiero and Wood, Simon N.},
	month = jul,
	year = {2011},
	pages = {2372--2387}
}