
# II: A review of Generalized Additive Models

```{r part_2_premble, include=FALSE}
#### Code for II: A review of Generalized Additive Models ####
```

The generalized linear model [GLM; @McCullagh:1989ti] relates the mean of a response ($y$) to a linear combination of explanatory variables. The response is assumed to be conditionally distributed according to some exponential family distribution (e.g., binomial, Poisson or Gamma distributions for trial, count or strictly positive real response, respectively). The generalized additive model [GAM; @Hastie:1990vg; @Ruppert:2003uc; @wood_generalized_2017] allows the relationships between the explanatory variables (henceforth covariates) and the response to be described by smooth curves (usually *splines* [@deBoor:1978wq], but potentially other structures). In general we have models of the form:
$$
\mathbb{E}\left( Y \right) = g^{-1}\left( \beta_0 + \sum_{j=1}^J f_j(x_j) \right)\,,
$$
where $\mathbb{E}(Y)$ is the expected value of the response $Y$ (with an appropriate distribution and link function $g$), $f_j$ is a smooth function of the covariate $x_j$, $\beta_0$ is an intercept term and $g^{-1}$ is the inverse link function. Hereafter, we will refer to these smooth functions as *smoothers*. In the example equation above, there are $J$ smoothers and each is a function of only one covariate, though it is possible to construct smoothers of multiple variables. 

Each smoother $f_j$ is represented by a sum of $K$ simpler, fixed *basis functions* ($b_{j,k}$) multiplied by corresponding coefficients ($\beta_{j,k}$), which need to be estimated:
$$
f_j(x_j) = \sum_{k=1}^K \beta_{j,k} b_{j,k}(x_j).
$$
$K$, referred to as "basis size", "basis complexity" or "basis richness", determines the maximum complexity of each smoother. 

It would seem that large basis size could lead to overfitting, but this counteracted by a *smoothing penalty* that influences basis function coefficients so as to prevent excess wiggliness and ensure that appropriate complexity of each smoother.  For each smoother, one or more *penalty matrices* $(\mathbf{S})$, specific to the form of the basis functions, is pre- and post-multiplied by the parameter vector $\boldsymbol{\beta}$ to calculate the penalty $(\boldsymbol{\beta}^T \mathbf{S} \boldsymbol{\beta})$.  A penalty term is then added to the model log-likelihood $L$, controlling the trade-off via a *smoothing parameter* ($\lambda$).  The penalized log-likelihood used to fit the model is thus:
$$
L - \boldsymbol{\lambda} \boldsymbol{\beta}^T \mathbf{S} \boldsymbol{\beta}
$$

Figure \ref{fig:smoothing_effect} shows an example of how different choices of the smoothing parameter ($\lambda$) affect the shape of the resulting smoother. Data (points) were generated from the blue function and noise added to them. In the left plot $\lambda$ was selected using Restricted Maximum Likelihood (REML) to give a good fit to the data, in the middle plot $\lambda$ was set to zero, so the penalty has no effect and the function interpolates the data, the right plot shows when $\lambda$ is set to a very large value, so the penalty removes all terms that have any wiggliness, giving a straight line.

To measure the complexity of a penalized smooth terms we use the *effective degrees of freedom* (EDF), which at a maximum is the number of coefficients to be estimated in the model, minus any constraints. The EDF can take non-integer values and larger values indicate more wiggly terms (see Wood [-@wood_generalized_2017, Section 6.1.2] for further details). The number of basis functions, $k$ sets a maximum for the EDF, as a smoother cannot have more than $k$ EDF. When the EDF is well below $k$, increasing $k$ generally has very little effect on the shape of the function. In general, $k$ should be set large enough to allow for potential variation in the smoother while still staying low enough to keep computation time low (see section V for more on this). 
In **mgcv**, the function `mgcv::check.gam` can be used to determine if $k$ has been set too low. 

```{r lambda, echo=FALSE, message=FALSE, results='hide', fig.width=8, fig.height=2.5, cache=TRUE, fig.cap="\\label{fig:smoothing_effect}Effect of different choices of smoothing parameter ($\\lambda$) on the shape of the resulting smoother. Left:  $\\lambda$ estimated using REML; middle: $\\lambda$ set to zero (no smoothing); Right: $\\lambda$ is set to a very large value.", messages=FALSE, dev=c('pdf'), out.width="\\linewidth"}
# example of varying lambda

set.seed(12)

# generate some data
dat <- gamSim(1, n=100, dist="normal", scale=2)
dat$y <- dat$y - (dat$f1 + dat$f0 + dat$f3)
dat$x <- dat$x2
true <- data.frame(x = sort(dat$x),
                   y = dat$f2[order(dat$x)])

## REML fit
b <- gam(y~s(x, k=100), data=dat, method = "REML")

# lambda=0
b.0 <- gam(y~s(x, k=100), data=dat, sp=0)

# lambda=infinity
b.inf <- gam(y~s(x, k=100), data=dat, sp=1e10)

# merging predictions from the models together to make plotting easier
pdat <- with(dat, data.frame(x = seq(min(x), max(x), length = 200)))
p <- cbind(pdat, fit = predict(b, newdata = pdat))
p.0 <- cbind(pdat, fit = predict(b.0, newdata = pdat))
p.inf <- cbind(pdat, fit = predict(b.inf, newdata = pdat))
ylims <- range(p, p.0, p.inf)

lab.l <- labs(x = "x", y = "y")
dat.l <- geom_point(data = dat, aes(x = x, y = y), colour = "darkgrey")
true.l <- geom_line(data = true, aes(x = x, y = y), colour = "blue")
coord.l <- coord_cartesian(ylim = ylims)

#plotting models
p1 <- ggplot(p, aes(x = x, y = fit)) +
  dat.l + true.l +
  geom_line(colour = "darkred") + lab.l + coord.l

p2 <- ggplot(p.0, aes(x = x, y = fit)) +
  dat.l + true.l +
  geom_line(colour = "darkred") + lab.l + coord.l

p3 <- ggplot(p.inf, aes(x = x, y = fit)) +
  dat.l + true.l +
  geom_line(colour = "darkred") + lab.l + coord.l

plot_grid(p1, p2, p3, align = "hv", axis = "lrtb", ncol = 3, labels = "auto")
```

Random effects are also "smooths" in this framework. In this case, the penalty matrix is the inverse of the correlation matrix of the basis function coefficients [@kimeldorf_correspondence_1970; @wood_generalized_2017]. To include a simple single-level random effect to account for variation in group means (intercepts) there will be one basis function for each level of the grouping variable, that takes a value of 1 for any observation in that group and 0 for any observation not in the group. The penalty matrix for these terms is a $n_g$ by $n_g$ identity matrix, where $n_g$ is the number of groups. This means that each group-level coefficient will be penalized in proportion to its squared deviation from zero. This is equivalent to how random effects are estimated in standard mixed effect models. The penalty term is then proportional to the inverse of the variance of the fixed effect estimated by standard hierarchical model software [@verbyla_analysis_2002].

This connection between random effects and splines extends beyond the varying-intercept case. Any single-penalty basis-function representation of a smooth can be transformed so that it can be represented as a combination of a random effect with an associated variance, and possibly one or more fixed effects. See @verbyla_analysis_2002 or @wood_straightforward_2012 for a more detailed discussion on the connections between these approaches.



### Basis types and penalty matrices

Different types of smoothers are useful for different needs, and have different associated penalty matrices for their basis function coefficients.  In the examples in this paper, we will use three types of smoothers: thin plate regression splines, cyclic cubic smoothers, and random effects. 

Thin plate regression splines [TPRS; @wood_thin_2003], are a general purpose spline basis which can be used for problems in any number of dimensions, provided one can assume that the amount of smoothing in any of the covariates is the same (so called isotropy or rotational invariance).  TPRS, like many splines, use a penalty matrix made up of terms based on the the integral of the squared derivatives of basis functions across their range (see @wood_generalized_2017 page 216 for details on this penalty). Models that overfit the data will tend to have large derivatives, so this penalization reduces wiggliness. We will refer to the order of penalized derivatives as $m$. Typically, TPRS are second-order ($m=2$), meaning that the penalty is proportionate to the integral of the squared second derivative. However, TPRS may be of lower order ($m=1$, penalizing squared first derivatives), or higher order ($m > 2$, penalizing squared higher order derivatives).  We will see in section III how lower-order TPRS smoothers are useful in fitting HGAMs. Example basis functions and penalty matrix $\mathbf{S}$ for a $m=2$ TPRS with six basis functions for evenly spaced data are shown in Fig. \ref{fig:basis_example}.

Cyclic cubic smoothers are another smoother that penalizes the squared second derivative of the smooth across the function. In these, though, start and end of the smoother are constrained to match in value and first derivative. These are useful for fitting models with cyclic components such as seasonal effects. We will use these smoothers to demonstrate how to fit HGAMs to cyclic data.


### Smoothing penalties vs. shrinkage penalties

Penalties can have two effects on how well a model fits: they can penalize how wiggly a given term is (smoothing) and they can penalize the absolute size of the function (shrinkage). The penalty can only affect the components of the smoother that have derivatives (the *range space*), not the other parts (the *null space*). For 1-dimensional thin plate regression splines (when  $m=2$), this means that there is a linear term left in the model, even when the penalty is in full force (as $\lambda \rightarrow \infty$), as shown in Fig. \ref{fig:basis_example} (this is also why Fig. \ref{fig:smoothing_effect}c resulted in a linear, rather than flat, fit to the data). The random effects smoother we discussed earlier is an example of a pure shrinkage penalty; it penalizes all deviations away from zero, no matter the pattern of those deviations. This will be useful later in section III, where we use random effect smoothers as one of the components of a HGAM.




```{r basis_function_examples, echo=FALSE, message=FALSE, results='hide', cache=TRUE, fig.width=8, fig.height= 7, fig.cap="\\label{fig:basis_example}a) Examples of the basis functions associated with a six basis function thin plate spline (m=2), calculated for data, $x$, spread evenly between $x=0$ and $x=1$. Each line represents a single basis function. b) The smoothing  penalty matrix for the thin plate smoother. Red entries indicate positive values and blue indicate negative values.  For example, functions F3 and F4 would have the greatest proportionate effect on the total penalty (as they have the largest values on the diagonal), whereas function F5 and F6 would not contribute to the wiggliness penalty at all (all the values in the 5th and 6th row and column of the penalty matrix are zero). This means these functions are in the null space of the penalty matrix, and are treated as completely smooth. c) An example of how the basis functions add up to create a single smooth function. Thin coloured lines represent each basis function multiplied by a coefficient, and the solid black line is the sum of those basis functions.", messages=FALSE, dev=c('pdf'), out.width="\\linewidth"}

#Code for generating figure 3: examples of basis functions and splines ####
k = 6
plotting_data = data.frame(x = seq(0,1,length=100))

#This creates the basis functions for a thin-plate spline The absorb.cons=FALSE
#setting makes sure that smoothCon does not remove basis functions that have a
#non-zero sum (in this case, the intercept). Absorbing constraints would result
#in having less than k basis functions, which is why fitted terms in mgcv often
#have less than k maximum EDF.
tp_basis = smoothCon(s(x,bs="tp",k=k), data=plotting_data,
                     knots=NULL,absorb.cons=FALSE)[[1]]

#### Extract basis functions #### 
tp_basis_funcr = as.data.frame(tp_basis$X)
names(tp_basis_funcr) = paste("F",1:k, sep="")
tp_basis_funcr$x = plotting_data$x
tp_basis_funcr$model = "Thin plate spline"

spline_basis_funcr = gather(tp_basis_funcr,func,value, -x,-model )



##### Extract penalty matrices ####

tp_basis_P = as.data.frame(tp_basis$S[[1]])
tp_basis_P = tp_basis_P/max(tp_basis_P)
names(tp_basis_P) = paste("F",1:k, sep="")
tp_basis_P$basis_y = factor(paste("F",1:k, sep=""), 
                            levels= rev(paste("F",1:k, sep="")))
tp_basis_P$model = "Thin plate spline"
spline_basis_penalties = gather(tp_basis_P ,basis_x,value,
                                -basis_y,-model )

spline_basis_penalties$penalty_type = "Smoothness penalty"


##### Creating a random draw from the function ####
#ensure we can reproduce this example
set.seed(6) 
coef_sample = rmvn(n = 1, mu = rep(0, times = k),V = 4*ginv(tp_basis$S[[1]]))

#randomly draw the null space terms from a normal distribution
coef_sample[(k-1):k] = c(-0.1,0.12)  
random_basis = as.matrix(tp_basis_funcr[,1:6])%*%diag(coef_sample) 
random_basis = as.data.frame(random_basis)
names(random_basis) = names(tp_basis_funcr)[1:6]

tp_example_curve = tp_basis_funcr %>%
  #remove the old basis functions
  select(-matches("F[1-9]")) %>% 
  #append the new basis functions multiplied by a random sample of coefficients
  bind_cols(random_basis) %>%
  mutate(Ftotal = rowSums(select(., matches("F[1-9]"))))%>%
  gather(key = `basis function`, value = value, matches("F[1-9]"))

  
bs_func_labels = tp_example_curve %>%
  group_by(`basis function`)%>%
  summarise(value = value[x==max(x)],
            x     = max(x))%>%
  mutate(coef = round(coef_sample,2),
         basis_label = paste("paste(",`basis function`,"%*%", coef, ")", 
                             sep=""))




#### Creating plots for smoothness and null-space penalties
#getting a color-blind palette for the 6 levels, avoiding the yellow and black 
#levels
basis_func_palette = colorblind_pal()(8)
basis_func_palette = basis_func_palette[-c(1,5)]

#Plotting the basis functions
basis_func_plot = ggplot(aes(x=x,y=value,color=func),data=spline_basis_funcr)+
  geom_line()+
  scale_x_continuous(breaks=seq(0,1,length=3),
                     labels=c("0","0.5","1"))+
  facet_wrap(~func)+
  scale_color_manual(values = basis_func_palette)+
  guides(color = "none")+
  theme_bw()+
  theme(strip.background = element_blank(), panel.grid = element_blank())

basis_penalty_plot = ggplot(aes(x=basis_x,y=basis_y,fill=value),
                            data=spline_basis_penalties)+
  geom_tile(color="black")+
  theme_bw()+
  scale_fill_gradient2("penalty",high = "#b2182b",low="#2166ac",midpoint = 0 )+
  theme(strip.background = element_blank())+
  labs(x="", y="")+
  coord_fixed()+
  scale_x_discrete(expand = c(0,0))+
  scale_y_discrete(expand = c(0,0))+
  theme(axis.ticks=element_blank(),
        panel.grid=element_blank())


basis_sample_plot = ggplot(data= tp_example_curve, aes(x, value))+
  geom_line(aes(y = Ftotal),size=2)+
  geom_line(aes(group = `basis function`,color= `basis function`),size=0.5)+ 
  geom_text(data= bs_func_labels,
            aes(label = basis_label, 
                color= `basis function`,
                y = value),
            parse=TRUE, hjust = 0,nudge_x = 0.01)+
  scale_x_continuous(expand = c(0,0),
                     limits = c(0,1.15))+
  scale_color_manual(values = basis_func_palette)+
  guides(color = "none")

#This is a nessecary step to make sure the top left plot is aligned with the 
#bottom plot. See https://cran.r-project.org/web/packages/cowplot/vignettes/plot_grid.html
#for details
aligned_plots = align_plots(basis_func_plot, basis_sample_plot, align = 'v', axis = 'l')

top_row_plot = plot_grid(aligned_plots[[1]],basis_penalty_plot,
                         ncol=2,
                         rel_widths = c(1.,0.9),labels= c("","b"))
  
full_plot = plot_grid(top_row_plot,aligned_plots[[2]],
                      nrow=2,
                      labels= c("a", "c"),
                      rel_heights = c(1,0.9))

full_plot
```


## Interactions between smooth terms

It is also possible to create interactions between covariates with different smoothers (or degrees of smoothness) assumed for each covariate, using *tensor products*. For instance, if one wanted to estimate the interacting effects of temperature and time (in seconds) on some outcome, it would not make sense to use a two-dimensional TPRS smoother, as that would assume that a one degree change in temperature would equate to a one second change in time. Instead, a tensor product allows us to create a new set of basis functions that allow for each marginal function (here temperature and time) to have its own marginal smoothness penalty. A different basis can be used in each marginal smooth, as required for the data at hand.

There are two approaches used in **mgcv** for generating tensor products. The first approach [@wood_lowrank_2006] essentially creates an interaction of each pair of basis functions for each marginal term, and a penalty for each marginal term that penalizes the average wiggliness in that term; in **mgcv**, these are created using the `te()` function. The second approach [@wood_straightforward_2012] separates each penalty into penalized (range space) and unpenalized components (null space; components that don't have derivatives, such as intercept and linear terms in a one-dimensional cubic spline), then creates new basis functions and penalties for all pair-wise combinations of penalized and unpenalized components between all pairs of marginal bases; in **mgcv**, these are created using the `t2()` function. The advantage of the first method is that it requires fewer smoothing parameters, so is faster to estimate in most cases. The advantage of the second method is that the tensor products created this way only have a single penalty associated with each marginal basis (unlike the `te()` approach, where each penalty applies to all basis functions), so it can be fitted using standard mixed effect software such as **lme4** [@bates_fitting_2015].

## Comparison to hierarchical linear models

Hierarchical generalized linear models [HGLMs; also referred to as generalized linear mixed effect models, multilevel models etc; e.g., @Bolker:2009cs; @Gelman:2006jh] are an extension of regression modelling that allows the inclusion of terms in the model that account for structure in the data --- the structure is usually of the form of a nesting of the observations. For example, in an empirical study, individuals may be nested within sample sites, sites are nested within forests, and forests within provinces. The depth of the nesting is limited by the fitting procedure and number of parameters to estimate.

HGLMs are a highly flexible way to think about grouping in ecological data; the groupings used in models often refer to the spatial or temporal scale of the data [@McMahon:2007ju] though can be based on any useful grouping.

We would like to be able to think about the groupings in our data in a similar way, even when the covariates in our model are related to the response in a smooth way. The next section investigates the extension of the smoothers we showed above to the case where observations are grouped and we model group-level smoothers.
